# Схемы пайплайнов работы агентов BettaFish

## Оглавление
1. [InsightEngine - Пайплайн анализа социальных медиа](#1-insightengine---пайплайн-анализа-социальных-медиа)
2. [QueryEngine - Пайплайн анализа новостей](#2-queryengine---пайплайн-анализа-новостей)
3. [MediaEngine - Пайплайн мультимодального анализа](#3-mediaengine---пайплайн-мультимодального-анализа)
4. [ReportEngine - Пайплайн генерации финального отчета](#4-reportengine---пайплайн-генерации-финального-отчета)
5. [ForumEngine - Пайплайн модерации мультиагентных дискуссий](#5-forumengine---пайплайн-модерации-мультиагентных-дискуссий)
6. [MindSpider - Пайплайн сбора данных](#6-mindspider---пайплайн-сбора-данных)

---

## 1. InsightEngine - Пайплайн анализа социальных медиа

**Назначение:** Анализ общественного мнения на основе локальной базы данных китайских социальных медиа (Bilibili, Weibo, Douyin, Kuaishou, Xiaohongshu, Zhihu, Tieba).

**Источник данных:** Локальная база данных PostgreSQL с собранным контентом

**Файл:** `InsightEngine/agent.py`

### 1.1 Общая схема пайплайна

```
┌──────────────────────────────────────────────────────────────────────┐
│                         INSIGHTENGINE PIPELINE                        │
└──────────────────────────────────────────────────────────────────────┘

INPUT: Пользовательский запрос (query)
  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ STEP 1: ГЕНЕРАЦИЯ СТРУКТУРЫ ОТЧЕТА                                  │
│ Node: ReportStructureNode                                            │
│ Prompt: SYSTEM_PROMPT_REPORT_STRUCTURE                              │
│                                                                       │
│ Задача: Создать структуру отчета из 5 параграфов                   │
│ Вход:  query (пользовательский запрос)                              │
│ Выход: List[Paragraph] - список из 5 параграфов                     │
│        - paragraph.title: заголовок параграфа                        │
│        - paragraph.content: описание содержания параграфа            │
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ STEP 2: ОБРАБОТКА КАЖДОГО ПАРАГРАФА (цикл for)                     │
│ Итерации: 5 раз (для каждого параграфа)                            │
└─────────────────────────────────────────────────────────────────────┘
  │
  ├─► ┌──────────────────────────────────────────────────────────────┐
  │   │ STEP 2.1: ПЕРВИЧНЫЙ ПОИСК И АНАЛИЗ                          │
  │   └──────────────────────────────────────────────────────────────┘
  │     │
  │     ├─► ┌────────────────────────────────────────────────────────┐
  │     │   │ 2.1.1: Генерация поискового запроса                    │
  │     │   │ Node: FirstSearchNode                                  │
  │     │   │ Prompt: SYSTEM_PROMPT_FIRST_SEARCH                     │
  │     │   │                                                          │
  │     │   │ Вход:                                                   │
  │     │   │   - title: заголовок параграфа                         │
  │     │   │   - content: описание содержания                       │
  │     │   │                                                          │
  │     │   │ Задача:                                                 │
  │     │   │   1. Выбрать инструмент из 6 доступных                 │
  │     │   │   2. Сформировать поисковый запрос на разговорном языке│
  │     │   │   3. Указать параметры (даты, платформа, лимиты)       │
  │     │   │                                                          │
  │     │   │ Выход (JSON):                                           │
  │     │   │   - search_tool: название инструмента                  │
  │     │   │   - search_query: поисковый запрос                     │
  │     │   │   - reasoning: обоснование выбора                      │
  │     │   │   - start_date (опц): дата начала                      │
  │     │   │   - end_date (опц): дата окончания                     │
  │     │   │   - platform (опц): платформа                          │
  │     │   │                                                          │
  │     │   │ Доступные инструменты:                                  │
  │     │   │   1. search_hot_content - горячий контент              │
  │     │   │   2. search_topic_globally - глобальный поиск          │
  │     │   │   3. search_topic_by_date - поиск по датам             │
  │     │   │   4. get_comments_for_topic - комментарии              │
  │     │   │   5. search_topic_on_platform - поиск по платформе     │
  │     │   │   6. analyze_sentiment - анализ эмоций                 │
  │     │   └────────────────────────────────────────────────────────┘
  │     │     │
  │     │     ▼
  │     ├─► ┌────────────────────────────────────────────────────────┐
  │     │   │ 2.1.2: Оптимизация ключевых слов                       │
  │     │   │ Tool: KeywordOptimizer                                 │
  │     │   │ File: InsightEngine/tools/keyword_optimizer.py        │
  │     │   │                                                          │
  │     │   │ Задача:                                                 │
  │     │   │   Преобразовать формальный запрос агента в             │
  │     │   │   разговорные ключевые слова                           │
  │     │   │                                                          │
  │     │   │ Вход:                                                   │
  │     │   │   - original_query: оригинальный запрос                │
  │     │   │                                                          │
  │     │   │ Принципы оптимизации:                                   │
  │     │   │   ❌ "舆情" (общественное мнение)                       │
  │     │   │   ✅ "大家怎么看" (что все думают)                      │
  │     │   │   ❌ "传播态势" (тенденция распространения)            │
  │     │   │   ✅ "火了" (стало горячим)                            │
  │     │   │                                                          │
  │     │   │ Выход (JSON):                                           │
  │     │   │   - keywords: List[str] (10-20 оптимизированных слов)  │
  │     │   │   - reasoning: обоснование выбора                      │
  │     │   │                                                          │
  │     │   │ Пример:                                                 │
  │     │   │   Вход: "武汉大学舆情管理未来展望发展趋势"              │
  │     │   │   Выход: ["武大", "武汉大学", "学校管理", "大学"]      │
  │     │   └────────────────────────────────────────────────────────┘
  │     │     │
  │     │     ▼
  │     ├─► ┌────────────────────────────────────────────────────────┐
  │     │   │ 2.1.3: Выполнение поиска в БД                          │
  │     │   │ Tool: MediaCrawlerDB                                   │
  │     │   │ File: InsightEngine/tools/search.py                   │
  │     │   │                                                          │
  │     │   │ Для каждого оптимизированного ключевого слова:         │
  │     │   │   1. Выполнить запрос к БД с выбранным инструментом    │
  │     │   │   2. Собрать результаты (контент + метаданные)         │
  │     │   │   3. Применить лимиты из конфигурации                  │
  │     │   │                                                          │
  │     │   │ Результаты включают:                                    │
  │     │   │   - title_or_content: текст контента                   │
  │     │   │   - platform: платформа (bilibili/weibo/...)           │
  │     │   │   - author_nickname: автор                             │
  │     │   │   - publish_time: время публикации                     │
  │     │   │   - url: ссылка                                        │
  │     │   │   - hotness_score: показатель популярности             │
  │     │   │   - engagement: вовлеченность (лайки/комменты/репосты) │
  │     │   │                                                          │
  │     │   │ Дедупликация: удаление дубликатов по URL               │
  │     │   └────────────────────────────────────────────────────────┘
  │     │     │
  │     │     ▼
  │     ├─► ┌────────────────────────────────────────────────────────┐
  │     │   │ 2.1.4: Анализ эмоций (опционально)                     │
  │     │   │ Tool: MultilinkualSentimentAnalyzer                    │
  │     │   │ File: InsightEngine/tools/sentiment_analyzer.py       │
  │     │   │                                                          │
  │     │   │ Если enable_sentiment = True (по умолчанию):           │
  │     │   │                                                          │
  │     │   │ Модель: WeiboMultilingualSentiment                     │
  │     │   │ Поддержка: 22 языка                                    │
  │     │   │ Классификация: 5 уровней                               │
  │     │   │   - очень негативный (0)                               │
  │     │   │   - негативный (1)                                     │
  │     │   │   - нейтральный (2)                                    │
  │     │   │   - позитивный (3)                                     │
  │     │   │   - очень позитивный (4)                               │
  │     │   │                                                          │
  │     │   │ Выход для каждого текста:                              │
  │     │   │   - sentiment_label: метка эмоции                      │
  │     │   │   - sentiment_score: уровень (0-4)                     │
  │     │   │   - confidence: уверенность модели (0-1)               │
  │     │   │                                                          │
  │     │   │ Агрегированная статистика:                             │
  │     │   │   - sentiment_distribution: распределение по категориям│
  │     │   │   - average_sentiment_score: средний показатель        │
  │     │   │   - high_confidence_count: кол-во с высокой уверенностью│
  │     │   │   - dominant_sentiment: доминирующая эмоция            │
  │     │   └────────────────────────────────────────────────────────┘
  │     │     │
  │     │     ▼
  │     ├─► ┌────────────────────────────────────────────────────────┐
  │     │   │ 2.1.5: Генерация первичного резюме                     │
  │     │   │ Node: FirstSummaryNode                                 │
  │     │   │ Prompt: SYSTEM_PROMPT_FIRST_SUMMARY                    │
  │     │   │                                                          │
  │     │   │ Вход:                                                   │
  │     │   │   - title: заголовок параграфа                         │
  │     │   │   - content: описание содержания                       │
  │     │   │   - search_query: использованный запрос                │
  │     │   │   - search_results: результаты поиска (форматированные)│
  │     │   │     + включает sentiment_analysis если доступно        │
  │     │   │                                                          │
  │     │   │ Задача:                                                 │
  │     │   │   Создать информационно-насыщенный параграф анализа    │
  │     │   │   объемом 800-1200 слов                                │
  │     │   │                                                          │
  │     │   │ Требования:                                             │
  │     │   │   - Минимум 5-8 цитат пользователей                    │
  │     │   │   - Распределение эмоций (позитивные X%, негативные Y%)│
  │     │   │   - Многоуровневый анализ (явления → данные → инсайты) │
  │     │   │   - 1-2 точки данных на каждые 100 слов                │
  │     │   │                                                          │
  │     │   │ Структура:                                              │
  │     │   │   ## Ключевые находки                                   │
  │     │   │   ## Детальная презентация данных                       │
  │     │   │   ## Репрезентативные голоса                            │
  │     │   │   ## Глубокая интерпретация                             │
  │     │   │   ## Тренды и характеристики                            │
  │     │   │                                                          │
  │     │   │ Выход (JSON):                                           │
  │     │   │   - paragraph_latest_state: текст резюме параграфа     │
  │     │   └────────────────────────────────────────────────────────┘
  │     │
  │     ▼
  ├─► ┌──────────────────────────────────────────────────────────────┐
  │   │ STEP 2.2: РЕФЛЕКСИВНЫЙ ЦИКЛ (повторяется N раз)             │
  │   │ Iterations: config.MAX_REFLECTIONS (обычно 1-3 раза)        │
  │   └──────────────────────────────────────────────────────────────┘
  │     │
  │     ├─► ┌────────────────────────────────────────────────────────┐
  │     │   │ 2.2.1: Рефлексивный анализ                             │
  │     │   │ Node: ReflectionNode                                   │
  │     │   │ Prompt: SYSTEM_PROMPT_REFLECTION                       │
  │     │   │                                                          │
  │     │   │ Вход:                                                   │
  │     │   │   - title: заголовок параграфа                         │
  │     │   │   - content: описание содержания                       │
  │     │   │   - paragraph_latest_state: текущее состояние параграфа│
  │     │   │                                                          │
  │     │   │ Задача:                                                 │
  │     │   │   Выявить пробелы в информации и определить            │
  │     │   │   необходимость дополнительного поиска                 │
  │     │   │                                                          │
  │     │   │ Анализирует:                                            │
  │     │   │   - Отсутствующие платформы (нет мнений с Bilibili?)   │
  │     │   │   - Временные периоды (нужны данные за другие даты?)   │
  │     │   │   - Точки зрения (не хватает определенных мнений?)     │
  │     │   │   - "Человечность" контента (слишком формально?)       │
  │     │   │                                                          │
  │     │   │ Выход (JSON):                                           │
  │     │   │   - search_tool: инструмент для дополнительного поиска │
  │     │   │   - search_query: новый запрос (разговорный язык!)     │
  │     │   │   - reasoning: обоснование необходимости доп. поиска   │
  │     │   │   - параметры (start_date, end_date, platform)         │
  │     │   └────────────────────────────────────────────────────────┘
  │     │     │
  │     │     ▼
  │     ├─► ┌────────────────────────────────────────────────────────┐
  │     │   │ 2.2.2: Дополнительный поиск                            │
  │     │   │ (повторяет шаги 2.1.2 - 2.1.4)                         │
  │     │   │                                                          │
  │     │   │ - KeywordOptimizer → оптимизация запроса               │
  │     │   │ - MediaCrawlerDB → поиск в БД                          │
  │     │   │ - SentimentAnalyzer → анализ эмоций                    │
  │     │   └────────────────────────────────────────────────────────┘
  │     │     │
  │     │     ▼
  │     ├─► ┌────────────────────────────────────────────────────────┐
  │     │   │ 2.2.3: Расширенное резюме                              │
  │     │   │ Node: ReflectionSummaryNode                            │
  │     │   │ Prompt: SYSTEM_PROMPT_REFLECTION_SUMMARY               │
  │     │   │                                                          │
  │     │   │ Вход:                                                   │
  │     │   │   - title: заголовок параграфа                         │
  │     │   │   - content: описание содержания                       │
  │     │   │   - search_query: запрос из рефлексии                  │
  │     │   │   - search_results: новые результаты поиска            │
  │     │   │   - paragraph_latest_state: предыдущая версия параграфа│
  │     │   │                                                          │
  │     │   │ Задача:                                                 │
  │     │   │   Значительно обогатить и углубить параграф до         │
  │     │   │   1000-1500 слов                                       │
  │     │   │                                                          │
  │     │   │ Требования:                                             │
  │     │   │   - Сохранить 70% оригинального контента               │
  │     │   │   - Добавить минимум 100% нового контента              │
  │     │   │   - 8-12 цитат пользователей                           │
  │     │   │   - Детальный анализ эмоций с сравнением               │
  │     │   │   - 3-5 точек данных на каждые 200 слов                │
  │     │   │                                                          │
  │     │   │ Структура расширенного анализа:                         │
  │     │   │   ### Ключевые находки (обновленная версия)            │
  │     │   │   ### Детальный портрет данных                          │
  │     │   │   ### Сведение разнообразных голосов                    │
  │     │   │   ### Апгрейд глубоких инсайтов                         │
  │     │   │   ### Распознавание трендов и паттернов                 │
  │     │   │   ### Сравнительный анализ                              │
  │     │   │                                                          │
  │     │   │ Выход (JSON):                                           │
  │     │   │   - updated_paragraph_latest_state: обогащенный текст  │
  │     │   └────────────────────────────────────────────────────────┘
  │     │
  │     ▼
  │   (Возврат к началу цикла рефлексии или завершение параграфа)
  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ STEP 3: ГЕНЕРАЦИЯ ФИНАЛЬНОГО ОТЧЕТА                                 │
│ Node: ReportFormattingNode                                           │
│ Prompt: SYSTEM_PROMPT_REPORT_FORMATTING                             │
│                                                                       │
│ Вход:                                                                │
│   - report_data: список всех 5 параграфов                           │
│     [                                                                 │
│       {                                                               │
│         "title": заголовок параграфа,                                │
│         "paragraph_latest_state": финальный текст параграфа         │
│       },                                                              │
│       ... (5 параграфов)                                             │
│     ]                                                                 │
│                                                                       │
│ Задача:                                                              │
│   Создать профессиональный отчет по анализу общественного мнения    │
│   объемом не менее 10 000 слов                                      │
│                                                                       │
│ Структура финального отчета:                                         │
│   # [舆情洞察] {тема} 深度舆情分析报告                              │
│                                                                       │
│   ## 执行摘要 (Executive Summary)                                    │
│   ### 核心舆情发现 (Core Findings)                                   │
│   ### 舆情热点概览 (Hotspots Overview)                               │
│                                                                       │
│   ## 1. [Параграф 1 Title]                                          │
│   ### 1.1 舆情数据画像 (Data Portrait)                               │
│   | 平台 | 参与用户 | 内容量 | 正面% | 负面% | 中性% |                │
│                                                                       │
│   ### 1.2 代表性民众声音 (Representative Voices)                     │
│   **支持声音 (XX%)**:                                                 │
│   > "具体用户评论1" —— @UserA (点赞: XXXX)                           │
│                                                                       │
│   **反对声音 (XX%)**:                                                 │
│   > "具体用户评论2" —— @UserB (评论: XXXX)                           │
│                                                                       │
│   ### 1.3 深度舆情解读 (Deep Interpretation)                         │
│   ### 1.4 情感演变轨迹 (Sentiment Evolution)                         │
│                                                                       │
│   ... (для каждого из 5 параграфов)                                 │
│                                                                       │
│   ## 综合舆情态势分析 (Comprehensive Analysis)                       │
│   ### 整体舆情倾向                                                    │
│   ### 不同群体意见对比                                                │
│   ### 平台差异化分析                                                  │
│   ### 舆情发展预判                                                    │
│                                                                       │
│   ## 深度洞察与建议 (Deep Insights)                                  │
│   ### 社会心理分析                                                    │
│   ### 舆情管理建议                                                    │
│                                                                       │
│   ## 数据附录 (Data Appendix)                                        │
│   ### 关键舆情指标汇总                                                │
│   ### 重要用户评论集锦                                                │
│   ### 详细情感分析数据                                                │
│                                                                       │
│ Специальные требования:                                             │
│   - Эмоциональная визуализация (😊 😡 😢 🤔)                        │
│   - Цветовые концепции ("红色警戒区", "绿色安全区")                  │
│   - Температурные метафоры ("沸腾", "升温", "降温")                  │
│   - Обширное цитирование пользовательских голосов                    │
│   - Таблицы сравнения мнений                                         │
│   - Социально-психологические инсайты                                │
│                                                                       │
│ Выход: final_report (строка в формате Markdown, 10000+ слов)        │
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ STEP 4: СОХРАНЕНИЕ ОТЧЕТА                                           │
│                                                                       │
│ Файл: deep_search_report_{query}_{timestamp}.md                     │
│ Путь: config.OUTPUT_DIR (обычно: insight_engine_streamlit_reports/) │
│                                                                       │
│ Дополнительно (если SAVE_INTERMEDIATE_STATES = True):               │
│   Файл: state_{query}_{timestamp}.json                              │
│   Содержит: полное состояние агента с историей поиска               │
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
OUTPUT: Markdown отчет объемом 10 000+ слов
```

### 1.2 Поток данных между промптами

```
┌─────────────────────────────────────────────────────────────────────┐
│                        ПОТОК ДАННЫХ (DATA FLOW)                      │
└─────────────────────────────────────────────────────────────────────┘

PROMPT 1: SYSTEM_PROMPT_REPORT_STRUCTURE
├─ Вход:
│  └─ query: "武汉大学舆情分析"
│
└─ Выход → State.paragraphs (список параграфов)
   ├─ paragraphs[0]:
   │  ├─ title: "事件背景与概况"
   │  └─ content: "全面回顾事件起因、发展脉络、关键节点"
   ├─ paragraphs[1]:
   │  ├─ title: "舆情热度与传播分析"
   │  └─ content: "数据统计、平台分布、传播路径、影响范围"
   ... (5 параграфов)

──────────────────────────────────────────────────────────────────────

ДЛЯ КАЖДОГО ПАРАГРАФА (цикл):

  PROMPT 2: SYSTEM_PROMPT_FIRST_SEARCH
  ├─ Вход:
  │  ├─ title: paragraphs[i].title
  │  └─ content: paragraphs[i].content
  │
  └─ Выход → search_config
     ├─ search_tool: "search_topic_globally"
     ├─ search_query: "武大学生怎么看"
     ├─ reasoning: "使用全局搜索获取多平台用户声音"
     └─ параметры: {}

  ────────────────────────────────────────────────────────────────────

  TOOL: KeywordOptimizer
  ├─ Вход:
  │  └─ original_query: "武大学生怎么看"
  │
  └─ Выход → optimized_keywords
     ├─ keywords: ["武大", "武汉大学", "武大学生", "珞珈山", "樱花大道"]
     └─ reasoning: "使用学生常用简称和地标名称"

  ────────────────────────────────────────────────────────────────────

  TOOL: MediaCrawlerDB (для каждого keyword)
  ├─ Вход:
  │  ├─ tool_name: "search_topic_globally"
  │  ├─ topic: "武大" (первое ключевое слово)
  │  └─ limit_per_table: config.DEFAULT_SEARCH_TOPIC_GLOBALLY_LIMIT_PER_TABLE
  │
  └─ Выход → DBResponse
     ├─ results: [
     │    {
     │      title_or_content: "武大真的太美了！樱花季必打卡",
     │      platform: "xiaohongshu",
     │      author_nickname: "小红书用户123",
     │      publish_time: "2025-03-15T10:30:00",
     │      url: "https://...",
     │      hotness_score: 8542,
     │      engagement: {likes: 1234, comments: 567, shares: 89}
     │    },
     │    ... (более результатов)
     │  ]
     └─ results_count: 150 (после дедупликации)

  ────────────────────────────────────────────────────────────────────

  TOOL: MultilinkualSentimentAnalyzer
  ├─ Вход:
  │  └─ query_results: DBResponse.results (список результатов)
  │
  └─ Выход → sentiment_analysis
     ├─ sentiment_distribution: {
     │    "very_negative": 5,    # 3.3%
     │    "negative": 15,         # 10%
     │    "neutral": 30,          # 20%
     │    "positive": 70,         # 46.7%
     │    "very_positive": 30     # 20%
     │  }
     ├─ average_sentiment_score: 3.2 (позитивный)
     ├─ high_confidence_count: 120 (80%)
     └─ dominant_sentiment: "positive"

  ────────────────────────────────────────────────────────────────────

  PROMPT 3: SYSTEM_PROMPT_FIRST_SUMMARY
  ├─ Вход:
  │  ├─ title: paragraphs[i].title
  │  ├─ content: paragraphs[i].content
  │  ├─ search_query: "武大学生怎么看"
  │  └─ search_results: форматированные результаты + sentiment_analysis
  │     └─ "1. [xiaohongshu] 武大真的太美了！... (情感: 非常正面, 置信度: 0.95)"
  │         "2. [bilibili] 武大YYDS！... (情感: 正面, 置信度: 0.89)"
  │         ...
  │         "情感分析汇总:"
  │         "  - 正面情绪: 66.7% (100条)"
  │         "  - 负面情绪: 13.3% (20条)"
  │         "  - 中性情绪: 20% (30条)"
  │         "  - 主导情绪: 正面"
  │
  └─ Выход → summary (800-1200 слов)
     ├─ paragraph_latest_state: "## 核心发现\n\n武汉大学..."
     └─ (сохраняется в State.paragraphs[i].research.latest_summary)

  ════════════════════════════════════════════════════════════════════
  РЕФЛЕКСИВНЫЙ ЦИКЛ (повторяется MAX_REFLECTIONS раз)
  ════════════════════════════════════════════════════════════════════

  PROMPT 4: SYSTEM_PROMPT_REFLECTION
  ├─ Вход:
  │  ├─ title: paragraphs[i].title
  │  ├─ content: paragraphs[i].content
  │  └─ paragraph_latest_state: предыдущий summary (800-1200 слов)
  │
  ├─ Анализирует пробелы:
  │  ├─ "缺少bilibili平台年轻用户的观点"
  │  ├─ "需要补充近一周内的最新评论"
  │  └─ "应该增加更多学生的真实情感表达"
  │
  └─ Выход → reflection_config
     ├─ search_tool: "search_topic_on_platform"
     ├─ search_query: "武大vlog" (更разговорный запрос!)
     ├─ platform: "bilibili"
     ├─ start_date: "2025-03-10"
     ├─ end_date: "2025-03-17"
     └─ reasoning: "需要补充B站年轻群体的视频评论"

  ────────────────────────────────────────────────────────────────────

  (Повторяются: KeywordOptimizer → MediaCrawlerDB → SentimentAnalyzer)
  ... новые результаты поиска с Bilibili ...

  ────────────────────────────────────────────────────────────────────

  PROMPT 5: SYSTEM_PROMPT_REFLECTION_SUMMARY
  ├─ Вход:
  │  ├─ title: paragraphs[i].title
  │  ├─ content: paragraphs[i].content
  │  ├─ search_query: "武大vlog"
  │  ├─ search_results: новые результаты с Bilibili + sentiment
  │  └─ paragraph_latest_state: предыдущая версия (800-1200 слов)
  │
  ├─ Задача:
  │  ├─ Сохранить 70% оригинального контента
  │  ├─ Добавить 100%+ нового контента
  │  ├─ Интегрировать новые голоса с Bilibili
  │  └─ Обновить эмоциональный анализ
  │
  └─ Выход → enriched_summary (1000-1500 слов)
     ├─ updated_paragraph_latest_state: "## 核心发现（更新版）\n\n..."
     └─ (обновляет State.paragraphs[i].research.latest_summary)

──────────────────────────────────────────────────────────────────────

PROMPT 6: SYSTEM_PROMPT_REPORT_FORMATTING
├─ Вход:
│  └─ report_data: [
│       {
│         title: paragraphs[0].title,
│         paragraph_latest_state: paragraphs[0].research.latest_summary
│       },
│       ... (5 параграфов, каждый 1000-1500 слов)
│     ]
│     Итого: ~5000-7500 слов входных данных
│
├─ Задача:
│  ├─ Создать полный отчет 10 000+ слов
│  ├─ Добавить Executive Summary
│  ├─ Создать таблицы данных
│  ├─ Добавить эмоциональную визуализацию
│  ├─ Написать综合分析 (комплексный анализ)
│  ├─ Создать раздел深度洞察 (глубокие инсайты)
│  └─ Добавить数据附录 (приложение данных)
│
└─ Выход → final_report (10 000+ слов)
   └─ "# [舆情洞察] 武汉大学 深度舆情分析报告\n\n## 执行摘要\n\n..."
```

### 1.3 Конфигурационные параметры

```python
# InsightEngine/utils/config.py

# LLM конфигурация
INSIGHT_ENGINE_API_KEY: str       # API ключ для LLM
INSIGHT_ENGINE_MODEL_NAME: str    # Название модели (например, "deepseek-chat")
INSIGHT_ENGINE_BASE_URL: str      # URL API эндпоинта

# База данных конфигурация
POSTGRESQL_HOST: str               # Хост PostgreSQL
POSTGRESQL_PORT: int               # Порт PostgreSQL
POSTGRESQL_USER: str               # Пользователь PostgreSQL
POSTGRESQL_PASSWORD: str           # Пароль PostgreSQL
POSTGRESQL_DATABASE: str           # Имя базы данных

# Лимиты поиска (используются агентом, но не передаются в промпты)
DEFAULT_SEARCH_HOT_CONTENT_LIMIT: int = 100
DEFAULT_SEARCH_TOPIC_GLOBALLY_LIMIT_PER_TABLE: int = 30
DEFAULT_SEARCH_TOPIC_BY_DATE_LIMIT_PER_TABLE: int = 30
DEFAULT_GET_COMMENTS_FOR_TOPIC_LIMIT: int = 300
DEFAULT_SEARCH_TOPIC_ON_PLATFORM_LIMIT: int = 200

# Обработка контента
MAX_SEARCH_RESULTS_FOR_LLM: int = 0  # 0 = без ограничений, передать все
MAX_CONTENT_LENGTH: int = 1000       # Макс. длина одного результата для LLM

# Рефлексивный цикл
MAX_REFLECTIONS: int = 1             # Количество циклов рефлексии

# Выходные файлы
OUTPUT_DIR: str = "insight_engine_streamlit_reports"
SAVE_INTERMEDIATE_STATES: bool = True
```

---


## 2. QueryEngine - Пайплайн анализа новостей

**Назначение:** Анализ новостной информации из интернета в реальном времени для проверки фактов и разоблачения дезинформации.

**Источник данных:** Tavily News API (поиск новостей в реальном времени)

**Файл:** `QueryEngine/agent.py`

### 2.1 Общая схема пайплайна

QueryEngine имеет **идентичную структуру** с InsightEngine, но использует разные инструменты:

```
INPUT: query
  ↓
STEP 1: ReportStructureNode (макс. 5 параграфов для новостей)
  ↓
STEP 2: ДЛЯ КАЖДОГО ПАРАГРАФА:
  ├─ FirstSearchNode → выбор инструмента (6 новостных API)
  ├─ TavilyNewsAgency → поиск новостей (без KeywordOptimizer!)
  ├─ FirstSummaryNode → резюме 800-1200 слов
  └─ [Reflection Loop - обычно отключен (MAX_REFLECTIONS = 0)]
  ↓
STEP 3: ReportFormattingNode → отчет 10000+ слов
  ↓
STEP 4: Сохранение
  ↓
OUTPUT: Новостной аналитический отчет (Markdown)
```

### 2.2 Основные отличия от InsightEngine

| Аспект | InsightEngine | QueryEngine |
|--------|---------------|-------------|
| **Источник** | Локальная БД PostgreSQL | Tavily News API |
| **Инструменты** | 6 БД инструментов | 6 новостных API |
| **KeywordOptimizer** | ✅ Да | ❌ Нет |
| **SentimentAnalyzer** | ✅ Да (22 языка, 5 уровней) | ❌ Нет |
| **Дедупликация** | ✅ Ручная (по URL) | ❌ Не нужна (API фильтрует) |
| **Reflection Loop** | 1-3 итерации | 0 итераций (обычно) |
| **Фокус** | Общественное мнение, эмоции | Факты, проверка, хронология |
| **Стиль языка** | Разговорный ("武大", "火了") | Формальный, журналистский |

### 2.3 Инструменты поиска QueryEngine

```python
# QueryEngine/tools/search.py - TavilyNewsAgency

1. basic_search_news(query, max_results=7)
   - Быстрый базовый поиск новостей
   - Использование: общие запросы

2. deep_search_news(query)
   - Глубокий анализ с AI-резюме
   - Использование: сложные темы, требующие детального анализа

3. search_news_last_24_hours(query)
   - Новости за последние 24 часа
   - Использование: breaking news, срочные события

4. search_news_last_week(query)
   - Новости за последнюю неделю
   - Использование: недавние тренды

5. search_images_for_news(query)
   - Поиск изображений по теме
   - Использование: визуальная информация

6. search_news_by_date(query, start_date, end_date)
   - Поиск в диапазоне дат (YYYY-MM-DD)
   - Использование: исторический анализ
```

### 2.4 Поток данных (пример)

```
INPUT: "比特币价格突破10万美元"

STEP 1: ReportStructureNode
Output: [
  {title: "价格突破与市场反应", content: "..."},
  {title: "技术分析与历史对比", content: "..."},
  {title: "机构投资者动向", content: "..."}
]

STEP 2.1: FirstSearchNode (параграф 1)
Output: {
  search_tool: "search_news_last_24_hours",
  search_query: "Bitcoin price 100k breakthrough",
  reasoning: "需要最新市场反应"
}

STEP 2.2: TavilyNewsAgency.search_news_last_24_hours()
Output: [
  {
    title: "Bitcoin Surges Past $100,000 Mark",
    url: "https://...",
    content: "Bitcoin reached a historic...",
    published_date: "2025-03-17",
    score: 0.97
  },
  ... (7-10 результатов)
]

STEP 2.3: FirstSummaryNode
Output: "## 核心事件概述\n\n比特币在2025年3月17日突破..." (800-1200 слов)

... (повторить для всех параграфов)

STEP 3: ReportFormattingNode
Output: "# [深度调查] 比特币价格突破10万美元 综合新闻分析报告\n\n..." (10000+ слов)
```

---


## 3. MediaEngine - Пайплайн мультимодального анализа

**Назначение:** Мультимодальный анализ контента, интегрирующий текст, изображения, AI-резюме и структурированные данные.

**Источник данных:** Bocha Multimodal Search API (веб-поиск с мультимодальными результатами)

**Файл:** `MediaEngine/agent.py`

### 3.1 Общая схема пайплайна

MediaEngine также имеет **идентичную базовую структуру**, но специализируется на мультимодальном контенте:

```
INPUT: query
  ↓
STEP 1: ReportStructureNode (макс. 5 параграфов)
  ↓
STEP 2: ДЛЯ КАЖДОГО ПАРАГРАФА:
  ├─ FirstSearchNode → выбор инструмента (5 мультимодальных)
  ├─ BochaMultimodalSearch → поиск (веб + изображения + AI + данные)
  ├─ FirstSummaryNode → резюме 800-1200 слов (мультимодальная интеграция)
  └─ [Reflection Loop - 1-2 итерации]
      ├─ ReflectionNode
      ├─ BochaMultimodalSearch (дополнительный поиск)
      └─ ReflectionSummaryNode → 1000-1500 слов
  ↓
STEP 3: ReportFormattingNode → отчет 10000+ слов (панорамный мультимедийный)
  ↓
STEP 4: Сохранение
  ↓
OUTPUT: Панорамный мультимедийный анализ (Markdown)
```

### 3.2 Инструменты поиска MediaEngine

```python
# MediaEngine/tools/search.py - BochaMultimodalSearch

1. comprehensive_search(query, max_results=10)
   - Возвращает: webpages + images + AI summaries + follow-up suggestions
   - Использование: полное исследование темы

2. web_search_only(query, max_results=15)
   - Возвращает: только веб-страницы (без AI анализа)
   - Использование: быстрый поиск, снижение затрат

3. search_for_structured_data(query)
   - Возвращает: "модальные карточки" (погода, акции, курсы, энциклопедии)
   - Использование: запросы фактической информации

4. search_last_24_hours(query)
   - Возвращает: контент за последние 24 часа
   - Использование: breaking news, актуальные события

5. search_last_week(query)
   - Возвращает: контент за последнюю неделю
   - Использование: недавние тренды
```

### 3.3 Мультимодальные результаты

```python
# Структура BochaResponse

webpages: List[Webpage]
  ├─ name: заголовок страницы
  ├─ url: ссылка
  ├─ snippet: краткое описание
  └─ date_last_crawled: дата последнего сканирования

images: List[Image]  # опционально
  ├─ url: ссылка на изображение
  ├─ description: описание изображения
  ├─ source_url: источник
  └─ width/height: размеры

ai_summary: str  # опционально (только comprehensive_search)
  └─ AI-сгенерированное резюме результатов

follow_up_queries: List[str]  # опционально
  └─ Предлагаемые последующие запросы

structured_data: Dict  # опционально (search_for_structured_data)
  └─ Структурированные данные (погода, акции, курсы, и т.д.)
```

### 3.4 Специфика FirstSummaryNode для медиа

```
PROMPT: SYSTEM_PROMPT_FIRST_SUMMARY (MediaEngine версия)

Вход:
  - search_results: мультимодальные результаты
    ├─ webpages: текстовая информация
    ├─ images: визуальный контент
    ├─ ai_summary: AI-анализ
    └─ structured_data: данные (если есть)

Задача:
  Создать многомерный комплексный анализ 800-1200 слов,
  интегрирующий ВСЕ типы информации

Структура резюме:
  ## Всесторонний обзор информации
  ## Углубленный анализ текстового контента
  ## Интерпретация визуальной информации
     - Описание типов изображений
     - Визуальное воздействие
     - Коммуникационная эффективность
  ## Комплексный анализ данных
  ## Многомерные инсайты

Требования:
  - 2-3 точки информации из РАЗНЫХ источников на 100 слов
  - Детальная интерпретация визуального контента
  - Кросс-медийная валидация и сравнение
  - Органическое сочетание текста, изображений и данных
```

### 3.5 Специфика ReportFormattingNode для медиа

```
PROMPT: SYSTEM_PROMPT_REPORT_FORMATTING (MediaEngine версия)

Структура панорамного мультимедийного отчета:

# [全景分析] {тема} 多维度整合分析报告

## 全景概览 (Panoramic Overview)
### 多维度信息汇总
  - 文本信息核心发现 (текстовые находки)
  - 视觉内容关键洞察 (визуальные инсайты)
  - 数据趋势重要指标 (тренды данных)
  - 跨媒体关联分析 (кросс-медиа анализ)

### 信息来源分布图
  | Тип информации | % |
  | 网页文本内容    | XX% |
  | 图片视觉信息    | XX% |
  | 结构化数据      | XX% |
  | AI分析洞察     | XX% |

## 1. [Параграф 1]
### 1.1 多模态信息画像
  | 信息类型 | 数量 | 主要内容 | 情感 | 传播效果 | 影响指数 |
  | 文本内容 | XX  | XX主题   | XX  | XX      | XX/10   |
  | 图片内容 | XX  | XX类型   | XX  | XX      | XX/10   |
  | 数据信息 | XX  | XX指标   | 中性 | XX      | XX/10   |

### 1.2 深度视觉内容分析
  **图片类型分布**:
  - 新闻图片 (XX张): 展示事件现场，情感倾向客观中立
    • 代表性图片: "图片描述内容..." (传播热度: ★★★★☆)
    • 视觉冲击力: 强，主要展示XX场景
  
  - 用户创作 (XX张): 反映个人观点，情感表达多样
    • 代表性图片: "图片描述内容..." (互动数据: XX赞)
    • 创作特点: XX风格，传达XX情绪

### 1.3 文本与视觉整合分析
### 1.4 数据与内容交叉验证

## 跨媒体综合分析 (Cross-Media Analysis)
### 信息一致性评估
  | 维度     | 文本内容 | 图片内容 | 数据信息 | 一致性评分 |
  | 主题焦点 | XX      | XX      | XX      | XX/10     |
  | 情感倾向 | XX      | XX      | 中性     | XX/10     |
  | 传播度   | XX      | XX      | XX      | XX/10     |

### 多维度影响力对比
  **文本传播特征**:
    - 信息密度: 高，包含大量细节和观点
    - 理性程度: 高，逻辑性强
    - 传播深度: 深，适合深度讨论
  
  **视觉传播特征**:
    - 情感冲击力: 强，直观视觉效果
    - 传播速度: 快，易于快速理解
    - 记忆效果: 好，视觉印象深刻
  
  **数据信息特征**:
    - 精确度: 很高，客观可靠
    - 权威性: 强，基于事实
    - 参考价值: 高，支撑分析判断

### 整合效应分析
  [多种媒体形式组合产生的综合效果]

## 多维度洞察与预测
### 跨媒体趋势识别
### 传播效果评估
### 综合影响力评估

## 多媒体数据附录
### 图片内容汇总表
### 关键数据指标集
### 跨媒体关联分析图
### AI分析结果汇总
```

### 3.6 Сравнение трех движков

```
┌─────────────────────────────────────────────────────────────────────┐
│                    СРАВНЕНИЕ ТРЕХ АНАЛИТИЧЕСКИХ ДВИЖКОВ              │
└─────────────────────────────────────────────────────────────────────┘

┌──────────────────┬──────────────────┬──────────────────┬─────────────┐
│     Аспект       │  InsightEngine   │   QueryEngine    │ MediaEngine │
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ ИСТОЧНИК ДАННЫХ  │ Локальная БД     │ Tavily News API  │ Bocha API   │
│                  │ PostgreSQL       │ (новости)        │ (мультимод.)│
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ ТИПЫ КОНТЕНТА    │ • Социальные     │ • Новостные      │ • Веб-страни│
│                  │   медиа посты    │   статьи         │ • Изображени│
│                  │ • Комментарии    │ • Пресс-релизы   │ • AI-резюме │
│                  │ • Видео описания │                  │ • Структ.дан│
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ КОЛ-ВО           │ 6 БД инструментов│ 6 новостных API  │ 5 мультимод.│
│ ИНСТРУМЕНТОВ     │                  │                  │   API       │
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ KEYWORD          │ ✅ Да            │ ❌ Нет           │ ❌ Нет      │
│ OPTIMIZER        │ (разговорный)    │                  │             │
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ SENTIMENT        │ ✅ Да            │ ❌ Нет           │ ❌ Нет      │
│ ANALYZER         │ (22 языка,       │ (фокус на фактах)│ (нейтральн.)│
│                  │  5 уровней)      │                  │             │
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ REFLECTION       │ 1-3 итерации     │ 0 итераций       │ 1-2 итераци │
│ LOOP             │ (глубокий анализ)│ (свежесть важнее)│ (углубление)│
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ ФОКУС АНАЛИЗА    │ • Общественное   │ • Факты          │ • Многомерн.│
│                  │   мнение         │ • Хронология     │   интеграция│
│                  │ • Эмоции         │ • Проверка       │ • Визуальный│
│                  │ • Социальная     │ • Достоверность  │   анализ    │
│                  │   психология     │ • Дезинформация  │ • Кросс-медиа│
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ СТИЛЬ ЯЗЫКА      │ Разговорный      │ Формальный       │ Описательный│
│                  │ ("武大", "火了") │ журналистский    │ мультимедийн│
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ ВИЗУАЛИЗАЦИЯ     │ • Эмодзи 😊😡   │ • Таблицы фактов │ • Описание  │
│ В ОТЧЕТЕ         │ • Цветовые       │ • Хронология     │   изображений│
│                  │   концепции      │ • Источники      │ • Кросс-медиа│
│                  │ • Температурные  │                  │   таблицы   │
│                  │   метафоры       │                  │ • Scoring   │
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ ТИПИЧНЫЙ         │ • "武汉大学舆情" │ • "乌克兰战争"   │ • "AI技术应用"│
│ ЗАПРОС           │ • "小红书种草"   │ • "经济政策"     │ • "品牌视觉"│
│                  │ • "网红争议"     │ • "科技突破"     │ • "产品评测"│
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ ДЛИНА ОТЧЕТА     │ 10 000+ слов     │ 10 000+ слов     │ 10 000+ слов│
├──────────────────┼──────────────────┼──────────────────┼─────────────┤
│ ВЫХОДНАЯ         │ insight_engine_  │ query_engine_    │ media_engine│
│ ДИРЕКТОРИЯ       │ streamlit_reports│ streamlit_reports│_streamlit_  │
│                  │                  │                  │  reports    │
└──────────────────┴──────────────────┴──────────────────┴─────────────┘

КЛЮЧЕВОЕ РАЗЛИЧИЕ:
  InsightEngine  = ГЛУБИНА (общественное мнение, эмоции, социальная психология)
  QueryEngine    = АКТУАЛЬНОСТЬ (факты, новости, проверка достоверности)
  MediaEngine    = ШИРИНА (мультимодальная интеграция, панорамный взгляд)
```

---


## 4. ReportEngine - Пайплайн генерации финального отчета

**Назначение:** Объединение результатов трех аналитических движков (Insight, Query, Media) и журналов обсуждений форума в единый профессиональный HTML-отчет.

**Источник данных:** 
- Markdown отчеты от InsightEngine, QueryEngine, MediaEngine
- Журнал дискуссий от ForumEngine (forum.log)

**Файл:** `ReportEngine/agent.py`

### 4.1 Общая схема пайплайна

```
┌──────────────────────────────────────────────────────────────────────┐
│                         REPORTENGINE PIPELINE                         │
└──────────────────────────────────────────────────────────────────────┘

INPUT:
  ├─ query: оригинальный пользовательский запрос
  ├─ reports: List[str] - 3 отчета от движков
  │   ├─ reports[0]: QueryEngine отчет (Markdown, 10000+ слов)
  │   ├─ reports[1]: MediaEngine отчет (Markdown, 10000+ слов)
  │   └─ reports[2]: InsightEngine отчет (Markdown, 10000+ слов)
  └─ forum_logs: str - журнал дискуссий (forum.log)

  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ STEP 1: ВЫБОР ШАБЛОНА ОТЧЕТА                                        │
│ Node: TemplateSelectionNode                                          │
│ Prompt: SYSTEM_PROMPT_TEMPLATE_SELECTION                            │
│                                                                       │
│ Вход:                                                                │
│   - query: оригинальный запрос                                      │
│   - reports: 3 отчета от движков                                    │
│   - forum_logs: журнал дискуссий                                    │
│                                                                       │
│ Задача:                                                              │
│   Выбрать наиболее подходящий шаблон из 6 доступных типов          │
│                                                                       │
│ Доступные шаблоны:                                                   │
│   1. 企业品牌声誉分析报告模板                                         │
│      (Анализ репутации корпоративного бренда)                        │
│      - Использование: оценка имиджа бренда, здоровье активов        │
│      - Период: годовой/полугодовой обзор                             │
│      - Фокус: стратегический, глобальный анализ                      │
│                                                                       │
│   2. 市场竞争格局舆情分析报告模板                                      │
│      (Анализ конкурентного ландшафта рынка)                          │
│      - Использование: анализ конкурентов, позиционирование           │
│      - Фокус: сравнение, стратегия дифференциации                    │
│                                                                       │
│   3. 日常或定期舆情监测报告模板                                         │
│      (Регулярный мониторинг общественного мнения)                    │
│      - Использование: еженедельный/ежемесячный мониторинг            │
│      - Фокус: динамика, ключевые данные, ранние признаки рисков     │
│                                                                       │
│   4. 特定政策或行业动态舆情分析报告                                      │
│      (Анализ политики/отраслевой динамики)                           │
│      - Использование: релизы политики, регуляторные изменения        │
│      - Фокус: глубокая интерпретация, прогнозирование трендов        │
│                                                                       │
│   5. 社会公共热点事件分析报告模板 ⭐ (РЕКОМЕНДУЕМЫЙ ПО УМОЛЧАНИЮ)    │
│      (Анализ социальных общественных событий)                        │
│      - Использование: общественные горячие темы, культурные феномены │
│      - Фокус: социальная психология, релевантность для организации  │
│                                                                       │
│   6. 突发事件与危机公关舆情报告模板                                      │
│      (Отчет о кризисном управлении)                                  │
│      - Использование: breaking негативные события с потенциальным    │
│        вредом для организации                                        │
│      - Фокус: быстрое реагирование, оценка рисков, контроль ситуации│
│                                                                       │
│ Критерии выбора:                                                     │
│   1. Тип темы запроса                                               │
│   2. Срочность и своевременность                                     │
│   3. Глубина и широта анализа                                       │
│   4. Целевая аудитория и сценарии использования                      │
│                                                                       │
│ Выход (JSON):                                                        │
│   - template_name: название выбранного шаблона                       │
│   - template_content: содержание шаблона (Markdown)                 │
│   - selection_reason: обоснование выбора                            │
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ STEP 2: ГЕНЕРАЦИЯ HTML-ОТЧЕТА                                       │
│ Node: HTMLGenerationNode                                             │
│ Prompt: SYSTEM_PROMPT_HTML_GENERATION                               │
│                                                                       │
│ Вход:                                                                │
│   - query: оригинальный запрос                                      │
│   - query_engine_report: отчет QueryEngine (Markdown)               │
│   - media_engine_report: отчет MediaEngine (Markdown)               │
│   - insight_engine_report: отчет InsightEngine (Markdown)           │
│   - forum_logs: журнал дискуссий ForumEngine                        │
│   - selected_template: выбранный шаблон                             │
│                                                                       │
│ Задача:                                                              │
│   Создать полный HTML-отчет объемом не менее 30 000 слов,           │
│   интегрирующий результаты трех движков и журналы дискуссий         │
│                                                                       │
│ Процесс интеграции:                                                  │
│   1. Избежать дублирования контента                                 │
│      - Идентифицировать перекрывающиеся разделы                     │
│      - Объединить похожие находки                                   │
│      - Сохранить уникальные инсайты каждого движка                  │
│                                                                       │
│   2. Комбинировать данные обсуждений трех движков (forum_logs)     │
│      - Извлечь выступления INSIGHT агента                           │
│      - Извлечь выступления MEDIA агента                             │
│      - Извлечь выступления QUERY агента                             │
│      - Извлечь модерацию HOST                                       │
│      - Синтезировать различные перспективы                          │
│                                                                       │
│   3. Организовать контент по структуре шаблона                      │
│      - Следовать выбранному шаблону                                 │
│      - Адаптировать разделы к конкретному запросу                   │
│                                                                       │
│   4. Добавить визуализацию данных                                   │
│      - Chart.js графики (круговые, линейные, распределение)         │
│      - Интерактивные элементы                                       │
│                                                                       │
│ HTML Требования:                                                     │
│   1. Полная HTML структура:                                          │
│      - DOCTYPE, html, head, body теги                               │
│      - Responsive CSS стилизация                                    │
│      - JavaScript интерактивные функции                              │
│      - Оглавление в начале статьи (НЕ sidebar!)                     │
│                                                                       │
│   2. Красивый дизайн:                                                │
│      - Современный UI дизайн                                        │
│      - Разумная цветовая схема                                      │
│      - Четкая компоновка                                            │
│      - Совместимость с мобильными устройствами                       │
│      - НЕ использовать эффекты с разворачиванием контента           │
│        (отображать все сразу)                                        │
│                                                                       │
│   3. Визуализация данных:                                            │
│      - Chart.js для генерации графиков                              │
│      - Круговые диаграммы анализа эмоций                            │
│      - Линейные графики анализа трендов                             │
│      - Диаграммы распределения источников данных                     │
│      - Графики статистики активности форума                          │
│                                                                       │
│   4. Структура контента:                                             │
│      - Заголовок отчета и резюме                                    │
│      - Интеграция результатов анализа каждого движка                │
│      - Анализ данных форума                                         │
│      - Комплексные выводы и рекомендации                            │
│      - Приложение данных                                            │
│                                                                       │
│   5. Интерактивные функции:                                          │
│      - Навигация по оглавлению                                      │
│      - Сворачивание/разворачивание разделов                         │
│      - Интерактивные графики                                        │
│      - Кнопки печати и экспорта в PDF                               │
│      - Переключатель темной темы                                     │
│                                                                       │
│ CSS Требования:                                                      │
│   - Использовать современные CSS функции (Flexbox, Grid)            │
│   - Responsive дизайн, поддержка различных размеров экрана          │
│   - Элегантные анимационные эффекты                                 │
│   - Профессиональные цветовые схемы                                 │
│                                                                       │
│ JavaScript Требования:                                               │
│   - Рендеринг графиков Chart.js                                     │
│   - Логика взаимодействия страницы                                  │
│   - Функциональность экспорта                                        │
│   - Переключение темы                                               │
│                                                                       │
│ Выход:                                                               │
│   - html_content: полный HTML-код отчета (30 000+ слов)             │
│     (возвращается напрямую, без пояснений или дополнительного текста)│
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ STEP 3: СОХРАНЕНИЕ ОТЧЕТА                                           │
│                                                                       │
│ Файл: final_report_{query}_{timestamp}.html                         │
│ Путь: config.OUTPUT_DIR (обычно: reports/)                          │
│                                                                       │
│ Дополнительно:                                                       │
│   Файл: report_state_{query}_{timestamp}.json                       │
│   Содержит: метаданные (шаблон, время генерации, и т.д.)           │
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
OUTPUT: Профессиональный HTML-отчет (30 000+ слов)
```

### 4.2 Пример структуры HTML-отчета

```html
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>[综合分析] {query} - 深度舆情分析报告</title>
    
    <!-- Chart.js для графиков -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    
    <style>
        /* Современный CSS с Flexbox/Grid */
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }
        .container { max-width: 1200px; margin: 0 auto; padding: 20px; }
        .header { text-align: center; padding: 40px 0; }
        .toc { margin: 30px 0; padding: 20px; background: #f5f5f5; }
        .section { margin: 40px 0; }
        .chart-container { margin: 30px 0; }
        /* ... более 500 строк CSS ... */
    </style>
</head>
<body>
    <div class="container">
        <!-- Заголовок -->
        <div class="header">
            <h1>[综合分析] {query}</h1>
            <p class="subtitle">深度舆情分析报告</p>
            <p class="date">生成时间: 2025-03-17 10:30:00</p>
        </div>
        
        <!-- Оглавление (НЕ sidebar, а в начале статьи) -->
        <div class="toc">
            <h2>目录</h2>
            <ul>
                <li><a href="#executive-summary">执行摘要</a></li>
                <li><a href="#query-analysis">新闻分析 (QueryEngine)</a></li>
                <li><a href="#media-analysis">多媒体分析 (MediaEngine)</a></li>
                <li><a href="#insight-analysis">舆情分析 (InsightEngine)</a></li>
                <li><a href="#forum-discussion">多智能体讨论</a></li>
                <li><a href="#comprehensive">综合结论</a></li>
            </ul>
        </div>
        
        <!-- Резюме -->
        <div id="executive-summary" class="section">
            <h2>执行摘要</h2>
            <!-- Интегрированные ключевые находки от всех трех движков -->
        </div>
        
        <!-- QueryEngine результаты -->
        <div id="query-analysis" class="section">
            <h2>新闻分析视角 (QueryEngine)</h2>
            <!-- Контент от QueryEngine -->
            
            <!-- График временной линии новостей -->
            <div class="chart-container">
                <canvas id="newsTimelineChart"></canvas>
            </div>
        </div>
        
        <!-- MediaEngine результаты -->
        <div id="media-analysis" class="section">
            <h2>多媒体分析视角 (MediaEngine)</h2>
            <!-- Контент от MediaEngine -->
            
            <!-- График распределения типов контента -->
            <div class="chart-container">
                <canvas id="contentTypesChart"></canvas>
            </div>
        </div>
        
        <!-- InsightEngine результаты -->
        <div id="insight-analysis" class="section">
            <h2>公众舆情视角 (InsightEngine)</h2>
            <!-- Контент от InsightEngine -->
            
            <!-- Круговая диаграмма анализа эмоций -->
            <div class="chart-container">
                <canvas id="sentimentChart"></canvas>
            </div>
        </div>
        
        <!-- Дискуссия форума -->
        <div id="forum-discussion" class="section">
            <h2>多智能体讨论记录</h2>
            <div class="forum-log">
                <!-- Форматированный журнал дискуссий -->
                <div class="speech insight">
                    <span class="speaker">INSIGHT:</span>
                    <span class="content">根据社交媒体数据...</span>
                    <span class="timestamp">10:30:15</span>
                </div>
                <div class="speech media">
                    <span class="speaker">MEDIA:</span>
                    <span class="content">从多媒体内容来看...</span>
                    <span class="timestamp">10:30:45</span>
                </div>
                <div class="speech query">
                    <span class="speaker">QUERY:</span>
                    <span class="content">新闻报道显示...</span>
                    <span class="timestamp">10:31:10</span>
                </div>
                <div class="speech host">
                    <span class="speaker">HOST:</span>
                    <span class="content">综合各位的发言...</span>
                    <span class="timestamp">10:31:40</span>
                </div>
            </div>
        </div>
        
        <!-- Комплексные выводы -->
        <div id="comprehensive" class="section">
            <h2>综合结论与建议</h2>
            <!-- Интегрированные выводы -->
        </div>
        
        <!-- Приложение данных -->
        <div class="appendix">
            <h2>数据附录</h2>
            <!-- Детальные данные, таблицы, источники -->
        </div>
    </div>
    
    <script>
        // Chart.js визуализация
        // График эмоций
        const sentimentCtx = document.getElementById('sentimentChart').getContext('2d');
        new Chart(sentimentCtx, {
            type: 'pie',
            data: {
                labels: ['非常正面', '正面', '中性', '负面', '非常负面'],
                datasets: [{
                    data: [20, 45, 20, 10, 5],
                    backgroundColor: ['#4CAF50', '#8BC34A', '#FFC107', '#FF9800', '#F44336']
                }]
            }
        });
        
        // График временной линии новостей
        const timelineCtx = document.getElementById('newsTimelineChart').getContext('2d');
        new Chart(timelineCtx, {
            type: 'line',
            data: { /* ... */ }
        });
        
        // Интерактивные функции
        // - Навигация
        // - Переключение темы
        // - Экспорт в PDF
        // ... более 200 строк JavaScript ...
    </script>
</body>
</html>
```

### 4.3 Механизм проверки готовности входных файлов

ReportEngine ожидает создания новых файлов от трех движков перед началом генерации:

```python
# ReportEngine/agent.py - FileCountBaseline

BASELINE_SYSTEM:
  1. При инициализации: считает количество .md файлов в каждой директории
     - insight_engine_streamlit_reports/
     - media_engine_streamlit_reports/
     - query_engine_streamlit_reports/
  
  2. Сохраняет baseline в logs/report_baseline.json
     {
       "insight": 5,
       "media": 3,
       "query": 7
     }
  
  3. Периодически проверяет наличие НОВЫХ файлов:
     - Текущее количество > baseline?
     - Есть ли новый файл в каждой директории?
     - Существует ли forum.log?
  
  4. Когда все условия выполнены → начинает генерацию отчета

ПРОЦЕСС:
  check_input_files() → {
    ready: True/False,
    baseline_counts: {insight: 5, media: 3, query: 7},
    current_counts: {insight: 6, media: 4, query: 8},
    new_files_found: {insight: 1, media: 1, query: 1},
    missing_files: [],
    latest_files: {
      insight: "path/to/latest_insight_report.md",
      media: "path/to/latest_media_report.md",
      query: "path/to/latest_query_report.md",
      forum: "logs/forum.log"
    }
  }
```

### 4.4 Конфигурационные параметры

```python
# ReportEngine/utils/config.py

# LLM конфигурация
REPORT_ENGINE_API_KEY: str
REPORT_ENGINE_MODEL_NAME: str
REPORT_ENGINE_BASE_URL: str

# Директории
TEMPLATE_DIR: str = "ReportEngine/templates"  # Шаблоны отчетов
OUTPUT_DIR: str = "reports"                   # Выходные HTML отчеты
LOG_FILE: str = "logs/report_engine.log"      # Лог-файл

# Мониторинг входных файлов
BASELINE_FILE: str = "logs/report_baseline.json"
INPUT_DIRS: Dict[str, str] = {
    "insight": "insight_engine_streamlit_reports",
    "media": "media_engine_streamlit_reports",
    "query": "query_engine_streamlit_reports"
}
FORUM_LOG_PATH: str = "logs/forum.log"
```

---


## 5. ForumEngine - Пайплайн модерации мультиагентных дискуссий

**Назначение:** Реал-тайм мониторинг логов трех аналитических движков, извлечение их "выступлений" и генерация модерации от AI-хоста.

**Источник данных:** 
- logs/insight.log
- logs/media.log
- logs/query.log

**Файлы:** 
- `ForumEngine/monitor.py` - LogMonitor (мониторинг и извлечение)
- `ForumEngine/llm_host.py` - AI Host (генерация модерации)

### 5.1 Общая схема пайплайна

```
┌──────────────────────────────────────────────────────────────────────┐
│                         FORUMENGINE PIPELINE                          │
│                   (Работает параллельно с движками)                   │
└──────────────────────────────────────────────────────────────────────┘

ИНИЦИАЛИЗАЦИЯ:
  ├─ LogMonitor создается при старте приложения
  ├─ Запускается фоновый поток мониторинга
  ├─ Очищается logs/forum.log
  └─ Записывается стартовая метка времени

  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ ФАЗА 1: ОЖИДАНИЕ ТРИГГЕРА                                           │
│ Thread: monitor_logs() [фоновый, daemon=True]                       │
│                                                                       │
│ Мониторинг состояния:                                                │
│   - is_searching = False (режим ожидания)                           │
│   - Проверка каждую 1 секунду: рост файлов логов?                  │
│                                                                       │
│ Условие триггера:                                                    │
│   Обнаружена строка в любом из трех логов:                          │
│     - "FirstSummaryNode" OR                                         │
│     - "正在生成首次段落总结" OR                                       │
│     - "{Engine}.nodes.summary_node" (InsightEngine/MediaEngine/...)│
│                                                                       │
│ Действия при триггере:                                              │
│   1. is_searching = True                                            │
│   2. clear_forum_log()  # очистить и начать новую сессию           │
│   3. Записать в forum.log: "=== ForumEngine 监控开始 - {time} ===" │
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ ФАЗА 2: АКТИВНЫЙ МОНИТОРИНГ                                         │
│ Thread: monitor_logs() [продолжает работать]                        │
│                                                                       │
│ Для каждого лог-файла (insight.log, media.log, query.log):         │
│                                                                       │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 2.1: ДЕТЕКТИРОВАНИЕ НОВЫХ СТРОК                               │   │
│ │                                                                 │   │
│ │ Метод: read_new_lines(file_path, app_name)                    │   │
│ │                                                                 │   │
│ │ Механизм:                                                       │   │
│ │   - Отслеживание file_positions[app_name] (позиция в байтах)  │   │
│ │   - Если текущий размер > last_position:                       │   │
│ │     1. Seek to last_position                                   │   │
│ │     2. Читать новый контент                                    │   │
│ │     3. Обновить file_positions[app_name] = current_position    │   │
│ │                                                                 │   │
│ │ Результат: List[str] - новые строки лога                      │   │
│ └───────────────────────────────────────────────────────────────┘   │
│   │                                                                  │
│   ▼                                                                  │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 2.2: ФИЛЬТРАЦИЯ И ИЗВЛЕЧЕНИЕ КОНТЕНТА                         │   │
│ │                                                                 │   │
│ │ Метод: process_lines_for_json(lines, app_name)                │   │
│ │                                                                 │   │
│ │ ВАЖНО: ERROR BLOCK FILTERING                                   │   │
│ │ Если в логе встречается ERROR уровень:                        │   │
│ │   - Входит в "ERROR block" состояние                          │   │
│ │   - ОТКАЗЫВАЕТСЯ обрабатывать ВСЕ строки                      │   │
│ │   - Выходит из "ERROR block" только при INFO уровне           │   │
│ │                                                                 │   │
│ │ Идентификация целевых строк:                                   │   │
│ │   ✅ is_target_log_line(line) = True если:                     │   │
│ │      - Содержит "FirstSummaryNode" OR                         │   │
│ │      - Содержит "ReflectionSummaryNode" OR                    │   │
│ │      - Содержит "{Engine}.nodes.summary_node" OR              │   │
│ │      - Содержит "正在生成首次段落总结" OR                      │   │
│ │      - Содержит "正在生成反思总结"                             │   │
│ │   ❌ НЕ является ERROR уровнем                                 │   │
│ │   ❌ НЕ содержит ошибочные ключевые слова:                     │   │
│ │      "JSON解析失败", "JSON修复失败", "Traceback"              │   │
│ │                                                                 │   │
│ │ Извлечение JSON контента:                                      │   │
│ │   Если обнаружена строка "清理后的输出: {"                     │   │
│ │   → Начинается многострочный захват JSON                       │   │
│ │   → capturing_json[app_name] = True                           │   │
│ │   → json_buffer[app_name] = [line1, line2, ...]               │   │
│ │   → Продолжать до обнаружения "}" или "] }"                   │   │
│ │   → Парсинг JSON и извлечение полей:                           │   │
│ │      • "updated_paragraph_latest_state" (приоритет) OR        │   │
│ │      • "paragraph_latest_state"                                │   │
│ │                                                                 │   │
│ │ Очистка контента:                                              │   │
│ │   - Удаление timestamp: [HH:MM:SS] или YYYY-MM-DD HH:mm:ss.SSS│   │
│ │   - Удаление тегов: [INSIGHT], [MEDIA], [QUERY]               │   │
│ │   - Удаление prefix: "首次总结: ", "反思总结: ", "清理后的输出: "│   │
│ │   - Удаление множественных пробелов                            │   │
│ │                                                                 │   │
│ │ Результат: List[str] - очищенные выступления агента           │   │
│ └───────────────────────────────────────────────────────────────┘   │
│   │                                                                  │
│   ▼                                                                  │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 2.3: ЗАПИСЬ В FORUM.LOG                                        │   │
│ │                                                                 │   │
│ │ Метод: write_to_forum_log(content, source)                    │   │
│ │ Формат: [HH:MM:SS] [SOURCE] content                           │   │
│ │                                                                 │   │
│ │ Для каждого извлеченного выступления:                          │   │
│ │   write_to_forum_log(content, "INSIGHT" | "MEDIA" | "QUERY")  │   │
│ │                                                                 │   │
│ │ Особенности:                                                    │   │
│ │   - Thread-safe (использует Lock)                             │   │
│ │   - Конвертирует \n в \\n (вся запись в одной строке)        │   │
│ │   - Flush после каждой записи                                 │   │
│ │                                                                 │   │
│ │ Буферизация для HOST:                                          │   │
│ │   agent_speeches_buffer.append(log_line)                      │   │
│ └───────────────────────────────────────────────────────────────┘   │
│   │                                                                  │
│   ▼                                                                  │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 2.4: ТРИГГЕР HOST МОДЕРАЦИИ                                    │   │
│ │                                                                 │   │
│ │ Условие: len(agent_speeches_buffer) >= 5                      │   │
│ │                                                                 │   │
│ │ Если выполнено и is_host_generating == False:                 │   │
│ │   → Вызвать _trigger_host_speech() СИНХРОННО                  │   │
│ └───────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ ФАЗА 3: ГЕНЕРАЦИЯ МОДЕРАЦИИ HOST                                    │
│ Метод: _trigger_host_speech() [синхронный вызов]                   │
│ File: ForumEngine/llm_host.py                                       │
│                                                                       │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 3.1: ПОДГОТОВКА ДАННЫХ                                         │   │
│ │                                                                 │   │
│ │ Входные данные:                                                │   │
│ │   recent_speeches = agent_speeches_buffer[:5]  # первые 5     │   │
│ │                                                                 │   │
│ │ Формат каждого выступления:                                    │   │
│ │   "[HH:MM:SS] [SPEAKER] content"                              │   │
│ │                                                                 │   │
│ │ Пример:                                                         │   │
│ │   "[10:30:15] [INSIGHT] 根据社交媒体数据分析..."              │   │
│ │   "[10:30:45] [MEDIA] 从视觉内容来看..."                      │   │
│ │   "[10:31:10] [QUERY] 最新新闻报道显示..."                    │   │
│ │   "[10:31:35] [INSIGHT] 进一步的舆情分析表明..."              │   │
│ │   "[10:32:00] [MEDIA] 图片数据补充说明..."                    │   │
│ └───────────────────────────────────────────────────────────────┘   │
│   │                                                                  │
│   ▼                                                                  │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 3.2: ПОСТРОЕНИЕ ПРОМПТОВ                                       │   │
│ │                                                                 │   │
│ │ SYSTEM PROMPT:                                                 │   │
│ │ ─────────────────────────────────────────────────────────────  │   │
│ │ You are a forum moderator for a multi-agent public opinion    │   │
│ │ analysis system. Your responsibilities are:                    │   │
│ │                                                                 │   │
│ │ 1. Event Organization: Identify key events, people, time nodes│   │
│ │    from each agent's statements, organize timeline            │   │
│ │ 2. Guide Discussion: Based on statements, guide in-depth      │   │
│ │    discussion, explore underlying causes                       │   │
│ │ 3. Correct Errors: If factual errors or logical contradictions│   │
│ │    found, clearly point them out                               │   │
│ │ 4. Integrate Perspectives: Synthesize different viewpoints,   │   │
│ │    form comprehensive understanding                            │   │
│ │ 5. Trend Prediction: Analyze trends, propose risk points      │   │
│ │ 6. Advance Analysis: Propose new analytical perspectives      │   │
│ │                                                                 │   │
│ │ Agent Introductions:                                           │   │
│ │   - INSIGHT: Focuses on private opinion DB, historical patterns│   │
│ │   - MEDIA: Excels at multimodal content, visual communication │   │
│ │   - QUERY: Responsible for precise searches, real-time info   │   │
│ │                                                                 │   │
│ │ Speaking Requirements:                                         │   │
│ │   - Comprehensiveness: Control within 1000 words              │   │
│ │   - Clear Structure: Event org, perspective comparison, Q&A   │   │
│ │   - In-depth Analysis: Propose deep insights                  │   │
│ │   - Objective Neutrality: Analyze based on facts              │   │
│ │   - Forward-looking: Propose forward-looking suggestions      │   │
│ │                                                                 │   │
│ │ USER PROMPT:                                                   │   │
│ │ ─────────────────────────────────────────────────────────────  │   │
│ │ Recent Agent Speech Records:                                   │   │
│ │ [10:30:15] INSIGHT:                                            │   │
│ │ {content of speech 1}                                          │   │
│ │                                                                 │   │
│ │ [10:30:45] MEDIA:                                              │   │
│ │ {content of speech 2}                                          │   │
│ │ ...                                                             │   │
│ │                                                                 │   │
│ │ As the forum moderator, please conduct comprehensive analysis │   │
│ │ according to the following structure:                          │   │
│ │                                                                 │   │
│ │ I. Event Organization and Timeline Analysis                   │   │
│ │    - Identify key events, people, time nodes                  │   │
│ │    - Organize timeline, clarify causal relationships          │   │
│ │    - Point out key turning points                             │   │
│ │                                                                 │   │
│ │ II. Perspective Integration and Comparative Analysis          │   │
│ │    - Synthesize perspectives from INSIGHT, MEDIA, QUERY       │   │
│ │    - Point out consensus and divergences                      │   │
│ │    - Analyze information value and complementarity            │   │
│ │    - If errors or contradictions found, point out with reasons│   │
│ │                                                                 │   │
│ │ III. Deep-level Analysis and Trend Prediction                 │   │
│ │    - Analyze deep causes and influencing factors              │   │
│ │    - Predict trends, point out risk points and opportunities  │   │
│ │    - Propose aspects requiring special attention              │   │
│ │                                                                 │   │
│ │ IV. Question Guidance and Discussion Direction                │   │
│ │    - Propose 2-3 key questions for further exploration        │   │
│ │    - Provide specific suggestions for subsequent research     │   │
│ │    - Guide agents to focus on specific data dimensions        │   │
│ │                                                                 │   │
│ │ Please deliver comprehensive moderator statement (within 1000 │   │
│ │ words), including the above four parts with clear logic.      │   │
│ └───────────────────────────────────────────────────────────────┘   │
│   │                                                                  │
│   ▼                                                                  │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 3.3: ВЫЗОВ LLM API                                             │   │
│ │                                                                 │   │
│ │ Функция: generate_host_speech(recent_speeches)                │   │
│ │                                                                 │   │
│ │ API вызов:                                                     │   │
│ │   client.chat.completions.create(                             │   │
│ │     model=FORUM_ENGINE_MODEL_NAME,                            │   │
│ │     messages=[                                                 │   │
│ │       {"role": "system", "content": SYSTEM_PROMPT},           │   │
│ │       {"role": "user", "content": USER_PROMPT}                │   │
│ │     ],                                                          │   │
│ │     temperature=0.7,                                           │   │
│ │     max_tokens=2000                                            │   │
│ │   )                                                             │   │
│ │                                                                 │   │
│ │ Выход:                                                         │   │
│ │   host_speech: str (1000 слов модерации)                      │   │
│ └───────────────────────────────────────────────────────────────┘   │
│   │                                                                  │
│   ▼                                                                  │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 3.4: ЗАПИСЬ МОДЕРАЦИИ В FORUM.LOG                             │   │
│ │                                                                 │   │
│ │ write_to_forum_log(host_speech, "HOST")                       │   │
│ │                                                                 │   │
│ │ Формат:                                                         │   │
│ │   [10:32:30] [HOST] {host_speech}                             │   │
│ │                                                                 │   │
│ │ Очистка буфера:                                                │   │
│ │   agent_speeches_buffer = agent_speeches_buffer[5:]           │   │
│ │   (удалить обработанные 5 выступлений, оставить остальные)    │   │
│ │                                                                 │   │
│ │ Сброс флага:                                                   │   │
│ │   is_host_generating = False                                  │   │
│ └───────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ ФАЗА 4: ЗАВЕРШЕНИЕ СЕССИИ                                           │
│                                                                       │
│ Условия завершения:                                                  │
│   1. Файл лога уменьшился в размере (лог был очищен)               │
│      → Немедленное завершение сессии                                │
│      → is_searching = False                                         │
│      → Записать: "=== ForumEngine 论坛结束 - {time} ==="           │
│                                                                       │
│   2. Таймаут (7200 секунд = 2 часа без активности)                 │
│      → search_inactive_count >= 7200                                │
│      → is_searching = False                                         │
│      → Записать: "=== ForumEngine 论坛结束 - {time} ==="           │
│                                                                       │
│ После завершения:                                                    │
│   - Возврат в ФАЗУ 1 (ожидание нового триггера)                    │
│   - Сброс всех состояний:                                           │
│     • agent_speeches_buffer = []                                    │
│     • is_host_generating = False                                    │
│     • capturing_json = {}                                           │
│     • json_buffer = {}                                              │
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
OUTPUT: logs/forum.log (форматированный журнал дискуссий)
```

### 5.2 Формат forum.log

```
[10:30:00] [SYSTEM] === ForumEngine 监控开始 - 2025-03-17 10:30:00 ===

[10:30:15] [INSIGHT] 根据社交媒体数据分析，武汉大学相关话题在过去24小时内出现显著增长。微博平台上的讨论热度提升了320%，主要集中在以下几个方面：\n\n1. 校园管理政策调整引发学生群体广泛关注\n2. 樱花季临近，校外游客预约系统升级\n3. 学生会改选相关讨论\n\n情感分析显示，整体舆情倾向积极（67.3%），但也存在部分质疑声音（18.2%），主要关注点在于...[更多内容]

[10:30:45] [MEDIA] 从多媒体内容分析来看，武汉大学相关的视觉传播呈现以下特征：\n\n**图片内容分布**：\n- 校园风景照片占比最高（45%），主要为樱花大道、珞珈山等标志性景观\n- 学生活动照片次之（30%），体现校园生活丰富性\n- 官方宣传图片占15%\n\n**视觉情感倾向**：\n整体呈现积极、温馨的视觉风格，图片中的色彩以粉色（樱花）、绿色（校园）为主...[更多内容]

[10:31:10] [QUERY] 最新新闻报道显示，武汉大学近期有以下几个重要动态：\n\n1. 【官方发布】武汉大学于3月15日发布2025年樱花季游客预约通知，将采用新的线上预约系统...\n   - 来源：武汉大学官方网站\n   - 发布时间：2025-03-15 09:00\n   - 可信度：非常高\n\n2. 【媒体报道】多家主流媒体关注武大樱花季管理措施...\n   - 新华社：强调"科学管理，文明赏花"\n   - 人民日报：聚焦"高校开放与管理平衡"\n...[更多内容]

[10:31:35] [INSIGHT] 进一步的舆情分析表明，不同平台用户群体对武大相关话题的关注点存在差异：\n\n**B站用户（18-25岁为主）**：\n- 关注点：校园vlog、学习生活分享、樱花季攻略\n- 情感倾向：非常积极（85.6%正面）\n- 代表性评论："武大yyds！"、"梦想中的大学"\n\n**微博用户（更广泛年龄层）**：\n- 关注点：校园管理政策、游客预约、文明赏花\n- 情感倾向：整体积极但有理性讨论（72.1%正面）\n- 代表性评论：讨论如何平衡开放与管理...[更多内容]

[10:32:00] [MEDIA] 图片数据补充说明：通过对500张武大相关图片的视觉分析发现：\n\n**传播效果评估**：\n- 高传播力图片（>1000次转发）：主要为樱花全景图、师生互动场景\n- 中等传播力图片（100-1000次）：日常校园生活、食堂美食\n- 低传播力图片（<100次）：建筑细节、普通街景\n\n**情感共鸣分析**：\n美好、温馨、怀旧是主要情感共鸣点...[更多内容]

[10:32:30] [HOST] 【论坛主持人综合分析】\n\n## 一、事件梳理与时间线分析\n\n根据三位agent的发言，我们可以清晰地看到武汉大学近期舆情的完整图景：\n\n**时间线重建**：\n- 3月15日：官方发布樱花季预约通知（QUERY提供）\n- 3月15-16日：社交媒体讨论热度激增320%（INSIGHT数据）\n- 3月16-17日：多媒体内容大量传播（MEDIA分析）\n\n**关键转折点**：\n预约系统的升级是本轮舆情的主要触发点，引发了公众对"高校开放度"这一更深层话题的讨论。\n\n## 二、视角整合与对比分析\n\n**三个agent的视角互补性**：\n\n1. **INSIGHT agent**提供了社交媒体的民间声音\n   - 优势：真实反映公众情感，数据量大\n   - 发现：不同平台用户关注点差异明显\n   - B站年轻用户更关注"体验"，微博用户更关注"管理"\n\n2. **MEDIA agent**提供了视觉传播维度\n   - 优势：补充了文字之外的情感表达\n   - 发现：樱花美景图片的高传播力验证了"美好"是主旋律\n   - 视觉内容与文字舆情高度一致（都是积极为主）\n\n3. **QUERY agent**提供了官方和新闻维度\n   - 优势：权威性高，事实清晰\n   - 发现：官方措施及时，主流媒体整体正面报道\n\n**信息一致性评估**：\n三个数据源高度一致，都显示舆情整体积极，这增强了我们结论的可信度。唯一的分歧在于"质疑声音"的占比——INSIGHT提到18.2%的质疑，但MEDIA和QUERY中较少反映，可能是因为负面情绪在视觉和正式新闻中表达较少。\n\n## 三、深层分析与趋势预判\n\n**深层原因**：\n1. **季节性因素**：樱花季是武大的年度"高光时刻"，自然引发关注\n2. **管理创新**：新预约系统体现了"科技+管理"的现代化尝试\n3. **社会心理**：疫情后公众对"校园开放"这一话题格外敏感\n\n**趋势预判**：\n- **短期（1-2周）**：随着樱花盛开，舆情热度将继续上升，预计在3月下旬达到峰值\n- **潜在风险点**：\n  1. 如果预约系统出现技术故障，可能引发负面舆情\n  2. 如果游客管理不当（如拥挤、不文明行为），可能影响学校声誉\n- **机会点**：\n  这是武大展示"智慧校园"形象的好机会，如果管理得当，可以成为高校开放管理的典范案例\n\n## 四、问题引导与讨论方向\n\n基于以上分析，我提出以下问题供各位agent进一步探讨：\n\n**问题1**：INSIGHT agent提到的18.2%质疑声音具体指向什么？是对预约系统的技术层面质疑，还是对"限制游客"这一政策本身的质疑？建议深入挖掘这部分负面舆情的具体诉求。\n\n**问题2**：MEDIA agent，能否进一步分析"低传播力图片"的特征？了解什么样的内容传播效果不佳，有助于我们理解公众的真实兴趣点。\n\n**问题3**：QUERY agent，主流媒体的报道中是否有提及其他高校的类似做法？横向对比可以帮助我们判断武大的做法是否具有创新性或普遍性。\n\n**后续研究建议**：\n- INSIGHT：建议监控未来一周内舆情走势，特别关注樱花盛开期的实时反馈\n- MEDIA：建议收集游客实拍图片/视频，分析"实际体验"与"期待"的差距\n- QUERY：建议追踪后续新闻报道，特别是如果出现突发事件（如系统故障、安全事故）的舆情应对\n\n【主持人发言结束】

[10:33:05] [INSIGHT] 针对HOST提出的问题1，我进一步分析了18.2%质疑声音的构成...[继续讨论]

...

[11:45:30] [SYSTEM] === ForumEngine 论坛结束 - 2025-03-17 11:45:30 ===
```

### 5.3 关键技术细节

#### 5.3.1 ERROR Block Filtering (критически важно!)

```python
# ForumEngine/monitor.py

# Проблема: В логах могут быть ERROR уровни с JSON-подобным контентом,
#           которые НЕ являются реальными выступлениями агентов

# Решение: Отслеживание ERROR/INFO состояний

def process_lines_for_json(lines, app_name):
    for line in lines:
        log_level = get_log_level(line)  # Извлечь: INFO, ERROR, WARNING, ...
        
        if log_level == 'ERROR':
            # Войти в ERROR block
            in_error_block[app_name] = True
            # Если захватывался JSON, немедленно прервать
            if capturing_json[app_name]:
                capturing_json[app_name] = False
                json_buffer[app_name] = []
            continue  # Пропустить всю строку
        
        elif log_level == 'INFO':
            # Выйти из ERROR block
            in_error_block[app_name] = False
        
        # Если в ERROR block, пропустить ВСЁ
        if in_error_block[app_name]:
            continue
        
        # Обычная обработка (только если НЕ в ERROR block)
        ...
```

#### 5.3.2 Многострочный JSON Capture

```python
# Проблема: LLM выход может быть многострочным:
#   [10:30:15] [InsightEngine] 清理后的输出: {
#   [10:30:15]   "paragraph_latest_state": "## 核心发现\n\n武汉大学..."
#   [10:30:15] }

# Решение: State Machine для захвата

capturing_json[app_name] = False  # Не захватывается
json_buffer[app_name] = []        # Буфер пуст

# Когда встречается "清理后的输出: {"
if is_json_start_line(line):
    capturing_json[app_name] = True
    json_buffer[app_name] = [line]
    
    # Проверить: это single-line JSON?
    if line.strip().endswith("}"):
        content = extract_json_content([line])
        write_to_forum_log(content, app_name.upper())
        capturing_json[app_name] = False
        json_buffer[app_name] = []

# Последующие строки
elif capturing_json[app_name]:
    json_buffer[app_name].append(line)
    
    # Проверить конец (после очистки timestamp)
    cleaned_line = remove_timestamp(line).strip()
    if cleaned_line == "}" or cleaned_line == "] }":
        # Конец JSON, обработать
        content = extract_json_content(json_buffer[app_name])
        write_to_forum_log(content, app_name.upper())
        capturing_json[app_name] = False
        json_buffer[app_name] = []
```

#### 5.3.3 Thread Safety

```python
# Проблема: Несколько потоков могут писать в forum.log одновременно
#   - Фоновый поток мониторинга
#   - Синхронный вызов HOST генерации

# Решение: Threading Lock

write_lock = Lock()

def write_to_forum_log(content, source):
    with write_lock:  # Эксклюзивный доступ
        with open(forum_log_file, 'a', encoding='utf-8') as f:
            timestamp = datetime.now().strftime('%H:%M:%S')
            # Конвертировать \n в \\n для single-line записи
            content_one_line = content.replace('\n', '\\n')
            f.write(f"[{timestamp}] [{source}] {content_one_line}\n")
            f.flush()
```

### 5.4 Конфигурационные параметры

```python
# ForumEngine конфигурация

# LLM для HOST модератора
FORUM_ENGINE_API_KEY: str
FORUM_ENGINE_MODEL_NAME: str
FORUM_ENGINE_BASE_URL: str

# Директории логов
LOG_DIR: str = "logs"
MONITORED_LOGS: Dict[str, str] = {
    'insight': 'logs/insight.log',
    'media': 'logs/media.log',
    'query': 'logs/query.log'
}
FORUM_LOG_FILE: str = "logs/forum.log"

# Пороги
HOST_SPEECH_THRESHOLD: int = 5   # Каждые 5 выступлений → HOST
SEARCH_INACTIVE_TIMEOUT: int = 7200  # 2 часа без активности
MONITOR_INTERVAL: int = 1  # Проверка каждую 1 секунду
```

---


## 6. MindSpider - Пайплайн сбора данных

**Назначение:** Автоматический сбор контента из китайских социальных медиа платформ и сохранение в локальную базу данных PostgreSQL для последующего анализа InsightEngine.

**Источник данных:** 7 китайских социальных медиа платформ через MediaCrawler фреймворк

**Файл:** `MindSpider/main.py`

### 6.1 Общая схема пайплайна

```
┌──────────────────────────────────────────────────────────────────────┐
│                         MINDSPIDER PIPELINE                           │
│                    (Автономный процесс сбора данных)                  │
└──────────────────────────────────────────────────────────────────────┘

ИНИЦИАЛИЗАЦИЯ:
  ├─ Загрузить конфигурацию из .env
  ├─ Подключиться к PostgreSQL
  └─ Инициализировать MediaCrawler фреймворк

  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ ФАЗА 1: ИЗВЛЕЧЕНИЕ ШИРОКИХ ТЕМ                                      │
│ Module: BroadTopicExtraction                                         │
│ File: MindSpider/BroadTopicExtraction/main.py                       │
│                                                                       │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 1.1: Получение горячих новостей                               │   │
│ │ Tool: get_today_news.py                                        │   │
│ │                                                                 │   │
│ │ Источники:                                                     │   │
│ │   - Baidu热搜 (Baidu Hot Search)                              │   │
│ │   - 微博热搜 (Weibo Hot Search)                               │   │
│ │   - 知乎热榜 (Zhihu Hot List)                                  │   │
│ │   - 抖音热点 (Douyin Trending)                                │   │
│ │   - 36氪科技 (36Kr Tech News)                                 │   │
│ │                                                                 │   │
│ │ Метод: Веб-скрапинг с requests + BeautifulSoup               │   │
│ │                                                                 │   │
│ │ Выход: List[NewsItem]                                          │   │
│ │   - title: заголовок новости                                   │   │
│ │   - url: ссылка                                               │   │
│ │   - source: источник (baidu/weibo/zhihu/...)                  │   │
│ │   - hot_value: показатель популярности                        │   │
│ │   - timestamp: время сбора                                    │   │
│ │                                                                 │   │
│ │ Обычно: 100-200 новостей в день                               │   │
│ └───────────────────────────────────────────────────────────────┘   │
│   │                                                                  │
│   ▼                                                                  │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 1.2: Извлечение ключевых слов и анализ                        │   │
│ │ Tool: TopicExtractor                                           │   │
│ │ File: MindSpider/BroadTopicExtraction/topic_extractor.py      │   │
│ │                                                                 │   │
│ │ LLM: DeepSeek (via API)                                       │   │
│ │                                                                 │   │
│ │ Задача 1: Извлечение до 100 ключевых слов                    │   │
│ │   - Приоритет горячим темам с высоким уровнем обсуждения     │   │
│ │   - Избегание слишком широких/конкретных терминов            │   │
│ │   - Подходящие для поиска в социальных медиа                 │   │
│ │                                                                 │   │
│ │ Задача 2: Генерация аналитического резюме (150-300 слов)     │   │
│ │   - Краткое обобщение основных новостей дня                  │   │
│ │   - Анализ социальных явлений и трендов                      │   │
│ │   - Фокусные направления текущего внимания                   │   │
│ │                                                                 │   │
│ │ Пример промпта:                                               │   │
│ │ ─────────────────────────────────────────────────────────────  │   │
│ │ Please analyze the following {count} today's hot news items: │   │
│ │                                                                 │   │
│ │ News List:                                                     │   │
│ │ 1. [Baidu] 武汉大学樱花季开放预约                             │   │
│ │ 2. [Weibo] AI技术在教育领域的应用                             │   │
│ │ ...                                                             │   │
│ │                                                                 │   │
│ │ Task 1: Extract keywords (max 100)                            │   │
│ │ Task 2: Write news analysis summary (150-300 words)           │   │
│ │                                                                 │   │
│ │ Output format:                                                 │   │
│ │ {                                                               │   │
│ │   "keywords": ["武汉大学", "樱花季", "AI教育", ...],          │   │
│ │   "summary": "今日新闻主要聚焦于..."                          │   │
│ │ }                                                               │   │
│ │ ─────────────────────────────────────────────────────────────  │   │
│ │                                                                 │   │
│ │ Выход:                                                         │   │
│ │   - keywords: List[str] (обычно 50-100 ключевых слов)        │   │
│ │   - summary: str (аналитическое резюме)                       │   │
│ └───────────────────────────────────────────────────────────────┘   │
│   │                                                                  │
│   ▼                                                                  │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 1.3: Сохранение в БД                                           │   │
│ │ Tool: DatabaseManager                                          │   │
│ │                                                                 │   │
│ │ Таблицы:                                                       │   │
│ │   - daily_topics: дневные темы и резюме                       │   │
│ │   - extracted_keywords: ключевые слова с весами               │   │
│ │                                                                 │   │
│ │ Запись:                                                        │   │
│ │   INSERT INTO daily_topics (date, summary, keyword_count)     │   │
│ │   VALUES ('2025-03-17', '今日新闻主要聚焦于...', 78)          │   │
│ │                                                                 │   │
│ │   INSERT INTO extracted_keywords (date, keyword, weight)      │   │
│ │   VALUES ('2025-03-17', '武汉大学', 0.95), ...                │   │
│ └───────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
┌─────────────────────────────────────────────────────────────────────┐
│ ФАЗА 2: ГЛУБОКИЙ СБОР КОНТЕНТА ПО КЛЮЧЕВЫМ СЛОВАМ                  │
│ Module: DeepSentimentCrawling                                        │
│ File: MindSpider/DeepSentimentCrawling/main.py                      │
│                                                                       │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 2.1: Управление ключевыми словами                             │   │
│ │ Tool: KeywordManager                                           │   │
│ │                                                                 │   │
│ │ Загрузка ключевых слов из БД:                                 │   │
│ │   SELECT keyword FROM extracted_keywords                       │   │
│ │   WHERE date = CURRENT_DATE                                    │   │
│ │   ORDER BY weight DESC                                         │   │
│ │   LIMIT 100                                                     │   │
│ │                                                                 │   │
│ │ Создание очереди задач:                                        │   │
│ │   keyword_queue = Queue()                                      │   │
│ │   for keyword in keywords:                                     │   │
│ │       keyword_queue.put(keyword)                               │   │
│ └───────────────────────────────────────────────────────────────┘   │
│   │                                                                  │
│   ▼                                                                  │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 2.2: Запуск краулеров для каждой платформы                    │   │
│ │ Tool: PlatformCrawler (использует MediaCrawler)               │   │
│ │ File: MindSpider/DeepSentimentCrawling/platform_crawler.py    │   │
│ │                                                                 │   │
│ │ Для каждого ключевого слова, для каждой платформы:            │   │
│ │                                                                 │   │
│ │ ПЛАТФОРМЫ (7):                                                 │   │
│ │ ┌─────────────────────────────────────────────────────────┐   │   │
│ │ │ 1. Bilibili (哔哩哔哩)                                   │   │   │
│ │ │    Тип: Видео + комментарии                              │   │   │
│ │ │    Метод: API + cookies авторизация                      │   │   │
│ │ │    Поля: video_title, author, play_count, like_count,    │   │   │
│ │ │          comment_count, comments[], tags[], duration      │   │   │
│ │ └─────────────────────────────────────────────────────────┘   │   │
│ │                                                                 │   │
│ │ ┌─────────────────────────────────────────────────────────┐   │   │
│ │ │ 2. Weibo (微博)                                          │   │   │
│ │ │    Тип: Микроблог посты + комментарии                    │   │   │
│ │ │    Метод: Mobile API                                      │   │   │
│ │ │    Поля: text, author, created_at, like_count,            │   │   │
│ │ │          repost_count, comment_count, comments[], images[]│   │   │
│ │ └─────────────────────────────────────────────────────────┘   │   │
│ │                                                                 │   │
│ │ ┌─────────────────────────────────────────────────────────┐   │   │
│ │ │ 3. Douyin (抖音)                                         │   │   │
│ │ │    Тип: Короткие видео + комментарии                     │   │   │
│ │ │    Метод: Web scraping + Selenium                        │   │   │
│ │ │    Поля: desc, author, digg_count, comment_count,        │   │   │
│ │ │          share_count, video_url, music                    │   │   │
│ │ └─────────────────────────────────────────────────────────┘   │   │
│ │                                                                 │   │
│ │ ┌─────────────────────────────────────────────────────────┐   │   │
│ │ │ 4. Kuaishou (快手)                                       │   │   │
│ │ │    Тип: Короткие видео + комментарии                     │   │   │
│ │ │    Метод: API                                             │   │   │
│ │ │    Поля: caption, author, view_count, like_count         │   │   │
│ │ └─────────────────────────────────────────────────────────┘   │   │
│ │                                                                 │   │
│ │ ┌─────────────────────────────────────────────────────────┐   │   │
│ │ │ 5. Xiaohongshu (小红书)                                  │   │   │
│ │ │    Тип: Лайфстайл посты + комментарии                    │   │   │
│ │ │    Метод: API                                             │   │   │
│ │ │    Поля: title, desc, author, like_count, collect_count, │   │   │
│ │ │          images[], tags[], location                       │   │   │
│ │ └─────────────────────────────────────────────────────────┘   │   │
│ │                                                                 │   │
│ │ ┌─────────────────────────────────────────────────────────┐   │   │
│ │ │ 6. Zhihu (知乎)                                          │   │   │
│ │ │    Тип: Q&A + статьи + комментарии                       │   │   │
│ │ │    Метод: API                                             │   │   │
│ │ │    Поля: question, answer, author, voteup_count,         │   │   │
│ │ │          comment_count, created_time                      │   │   │
│ │ └─────────────────────────────────────────────────────────┘   │   │
│ │                                                                 │   │
│ │ ┌─────────────────────────────────────────────────────────┐   │   │
│ │ │ 7. Tieba (贴吧)                                          │   │   │
│ │ │    Тип: Форум посты + комментарии                        │   │   │
│ │ │    Метод: Web scraping                                    │   │   │
│ │ │    Поля: title, content, author, reply_num, create_time  │   │   │
│ │ └─────────────────────────────────────────────────────────┘   │   │
│ │                                                                 │   │
│ │ Для каждой платформы:                                          │   │
│ │   1. Выполнить поиск по ключевому слову                       │   │
│ │   2. Собрать топ N результатов (обычно 20-50)                 │   │
│ │   3. Для каждого поста: собрать комментарии (топ M, обычно 50)│   │
│ │   4. Нормализовать данные в единый формат                     │   │
│ │   5. Сохранить в БД                                            │   │
│ │                                                                 │   │
│ │ Анти-бан меры:                                                 │   │
│ │   - Rate limiting (delay между запросами)                     │   │
│ │   - User-Agent rotation                                        │   │
│ │   - Proxy rotation (опционально)                              │   │
│ │   - Cookie management                                          │   │
│ └───────────────────────────────────────────────────────────────┘   │
│   │                                                                  │
│   ▼                                                                  │
│ ┌───────────────────────────────────────────────────────────────┐   │
│ │ 2.3: Нормализация и сохранение в БД                           │   │
│ │                                                                 │   │
│ │ Таблицы PostgreSQL (7 платформ):                              │   │
│ │   - bilibili_posts, bilibili_comments                         │   │
│ │   - weibo_posts, weibo_comments                               │   │
│ │   - douyin_videos, douyin_comments                            │   │
│ │   - kuaishou_videos, kuaishou_comments                        │   │
│ │   - xiaohongshu_notes, xiaohongshu_comments                   │   │
│ │   - zhihu_answers, zhihu_comments                             │   │
│ │   - tieba_posts, tieba_replies                                │   │
│ │                                                                 │   │
│ │ Общие поля (нормализованные):                                 │   │
│ │   - id: уникальный ID (generated)                             │   │
│ │   - platform_id: ID на платформе                              │   │
│ │   - keyword: ключевое слово поиска                            │   │
│ │   - title_or_content: основной текст                          │   │
│ │   - author_nickname: автор                                    │   │
│ │   - publish_time: время публикации                            │   │
│ │   - url: ссылка на оригинал                                   │   │
│ │   - hotness_score: популярность (лайки + комменты + репосты) │   │
│ │   - engagement: детальные метрики                             │   │
│ │     {likes: X, comments: Y, shares: Z, views: W}              │   │
│ │   - created_at: время сбора                                   │   │
│ │   - updated_at: время обновления                              │   │
│ │                                                                 │   │
│ │ Пример:                                                        │   │
│ │ INSERT INTO bilibili_posts (                                  │   │
│ │   platform_id, keyword, title_or_content, author_nickname,    │   │
│ │   publish_time, url, hotness_score, engagement                │   │
│ │ ) VALUES (                                                     │   │
│ │   'BV1xx411c7mD', '武汉大学',                                  │   │
│ │   '武大樱花真的太美了！', '@UP主123',                          │   │
│ │   '2025-03-17 10:30:00',                                       │   │
│ │   'https://www.bilibili.com/video/BV1xx411c7mD',              │   │
│ │   15420, '{"likes": 12000, "comments": 3000, ...}'            │   │
│ │ )                                                               │   │
│ └───────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
  │
  ▼
OUTPUT: Локальная БД PostgreSQL (постоянное пополнение)
        → Используется InsightEngine для анализа
```

### 6.2 Архитектура базы данных

```sql
-- MindSpider/schema/models_bigdata.py → SQL

-- ═══════════════════════════════════════════════════════════════════
-- ТАБЛИЦЫ КЛЮЧЕВЫХ СЛОВ
-- ═══════════════════════════════════════════════════════════════════

CREATE TABLE daily_topics (
    id SERIAL PRIMARY KEY,
    date DATE NOT NULL UNIQUE,
    summary TEXT,                     -- Резюме дня от LLM
    keyword_count INTEGER,            -- Количество ключевых слов
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE extracted_keywords (
    id SERIAL PRIMARY KEY,
    date DATE NOT NULL,
    keyword VARCHAR(100) NOT NULL,
    weight FLOAT DEFAULT 1.0,         -- Вес ключевого слова (0-1)
    source VARCHAR(50),               -- Источник (baidu/weibo/...)
    created_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(date, keyword)
);

-- ═══════════════════════════════════════════════════════════════════
-- ТАБЛИЦЫ КОНТЕНТА (для каждой платформы)
-- ═══════════════════════════════════════════════════════════════════

-- 1. Bilibili
CREATE TABLE bilibili_posts (
    id SERIAL PRIMARY KEY,
    platform_id VARCHAR(50) UNIQUE,   -- BV号
    keyword VARCHAR(100),
    title_or_content TEXT,
    author_nickname VARCHAR(100),
    publish_time TIMESTAMP,
    url TEXT,
    hotness_score BIGINT,
    engagement JSONB,                 -- {likes, comments, shares, views}
    content_type VARCHAR(20),         -- 'post' or 'comment'
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE bilibili_comments (
    id SERIAL PRIMARY KEY,
    post_id INTEGER REFERENCES bilibili_posts(id),
    platform_comment_id VARCHAR(50),
    content TEXT,
    author_nickname VARCHAR(100),
    publish_time TIMESTAMP,
    like_count INTEGER,
    created_at TIMESTAMP DEFAULT NOW()
);

-- 2. Weibo
CREATE TABLE weibo_posts (
    id SERIAL PRIMARY KEY,
    platform_id VARCHAR(50) UNIQUE,
    keyword VARCHAR(100),
    title_or_content TEXT,
    author_nickname VARCHAR(100),
    publish_time TIMESTAMP,
    url TEXT,
    hotness_score BIGINT,
    engagement JSONB,
    images JSONB,                     -- Массив URL изображений
    content_type VARCHAR(20),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE weibo_comments (
    id SERIAL PRIMARY KEY,
    post_id INTEGER REFERENCES weibo_posts(id),
    platform_comment_id VARCHAR(50),
    content TEXT,
    author_nickname VARCHAR(100),
    publish_time TIMESTAMP,
    like_count INTEGER,
    created_at TIMESTAMP DEFAULT NOW()
);

-- 3-7. Аналогичные таблицы для Douyin, Kuaishou, Xiaohongshu, Zhihu, Tieba
-- ... (структура похожая, с небольшими вариациями полей)
```

### 6.3 Конфигурационные параметры

```python
# MindSpider/config.py

# PostgreSQL конфигурация
POSTGRESQL_HOST: str
POSTGRESQL_PORT: int
POSTGRESQL_USER: str
POSTGRESQL_PASSWORD: str
POSTGRESQL_DATABASE: str

# LLM для извлечения тем (TopicExtractor)
MINDSPIDER_API_KEY: str              # DeepSeek API key
MINDSPIDER_MODEL_NAME: str = "deepseek-chat"
MINDSPIDER_BASE_URL: str

# Краулинг параметры
CRAWL_BATCH_SIZE: int = 100          # Ключевых слов за раз
MAX_POSTS_PER_KEYWORD: int = 50      # Постов на ключевое слово
MAX_COMMENTS_PER_POST: int = 50      # Комментариев на пост
CRAWL_DELAY_SECONDS: int = 2         # Задержка между запросами

# MediaCrawler конфигурация
PLATFORMS: List[str] = [
    "bilibili", "weibo", "douyin", 
    "kuaishou", "xiaohongshu", "zhihu", "tieba"
]
HEADLESS_MODE: bool = True           # Браузер в headless режиме
SAVE_LOGIN_STATE: bool = True        # Сохранять cookies

# Расписание (опционально)
DAILY_CRAWL_TIME: str = "02:00"      # Ежедневный сбор в 2:00 AM
```

---

## 7. Итоговая схема взаимодействия всех пайплайнов

```
┌─────────────────────────────────────────────────────────────────────┐
│                    BETTAFISH ПОЛНАЯ СИСТЕМА                          │
│                   (Multi-Agent Sentiment Analysis)                   │
└─────────────────────────────────────────────────────────────────────┘

                          ПОЛЬЗОВАТЕЛЬ
                               │
                               │ query
                               ▼
                    ┌─────────────────────┐
                    │   MAIN ORCHESTRATOR │
                    │  (Streamlit UI/API) │
                    └─────────────────────┘
                               │
               ┌───────────────┼───────────────┐
               │               │               │
               ▼               ▼               ▼
    ┌──────────────┐ ┌──────────────┐ ┌──────────────┐
    │ INSIGHTENGINE│ │ QUERYENGINE  │ │ MEDIAENGINE  │
    │ (социальные  │ │  (новости)   │ │ (мультимедиа)│
    │    медиа)    │ │              │ │              │
    └──────────────┘ └──────────────┘ └──────────────┘
           │                 │                 │
           │ читает из       │ API запросы     │ API запросы
           │                 │                 │
           ▼                 ▼                 ▼
    ┌──────────────┐ ┌──────────────┐ ┌──────────────┐
    │ PostgreSQL БД│ │ Tavily News  │ │  Bocha API   │
    │ (локальная)  │ │    API       │ │              │
    └──────────────┘ └──────────────┘ └──────────────┘
           ▲
           │ заполняется
           │
    ┌──────────────┐
    │  MINDSPIDER  │ ◄───── Автономный процесс
    │ (Data Mining)│        (ежедневный сбор)
    └──────────────┘
           ▲
           │ краулит
           │
    ┌──────────────┐
    │ 7 Платформ:  │
    │ Bilibili     │
    │ Weibo        │
    │ Douyin       │
    │ Kuaishou     │
    │ Xiaohongshu  │
    │ Zhihu        │
    │ Tieba        │
    └──────────────┘

    ════════════════════════════════════════════════

    Параллельно работает:

    ┌──────────────┐
    │ FORUMENGINE  │ ◄───── Фоновый поток
    │ (Модератор)  │        (мониторинг логов)
    └──────────────┘
           │
           │ мониторит
           ▼
    ┌──────────────┐
    │  logs/       │
    │ insight.log  │
    │ media.log    │
    │ query.log    │
    └──────────────┘
           │
           │ извлекает выступления
           ▼
    ┌──────────────┐
    │ logs/        │
    │ forum.log    │ ───────┐
    └──────────────┘        │
                            │ читает
    ════════════════════════│═══════════════════════
                            │
                            ▼
                    ┌──────────────┐
                    │ REPORTENGINE │
                    │ (Финальный   │
                    │   отчет)     │
                    └──────────────┘
                            │
                            │ объединяет
                            │
         ┌──────────────────┼──────────────────┐
         │                  │                  │
         ▼                  ▼                  ▼
    Insight отчет     Query отчет       Media отчет
    (10K слов)        (10K слов)        (10K слов)
         │                  │                  │
         └──────────────────┴──────────────────┘
                            │
                            │ + forum.log
                            │
                            ▼
                    ┌──────────────┐
                    │  HTML ОТЧЕТ  │
                    │ (30K+ слов)  │
                    │              │
                    │ • Chart.js   │
                    │ • Interactive│
                    │ • Responsive │
                    └──────────────┘
                            │
                            ▼
                       ПОЛЬЗОВАТЕЛЬ
```

### 7.1 Хронология выполнения

```
T=0: ПОЛЬЗОВАТЕЛЬ → query: "武汉大学舆情分析"

T=0.1s:
  ├─ ForumEngine.start_monitoring() (если еще не запущен)
  └─ Запуск трех агентов параллельно:
      ├─ InsightEngine.research(query)
      ├─ QueryEngine.research(query)
      └─ MediaEngine.research(query)

T=1s-300s: (InsightEngine)
  ├─ ReportStructureNode → 5 параграфов
  ├─ Для каждого параграфа:
  │   ├─ FirstSearchNode → выбор инструмента
  │   ├─ KeywordOptimizer → оптимизация
  │   ├─ MediaCrawlerDB → поиск в БД
  │   ├─ SentimentAnalyzer → анализ эмоций
  │   ├─ FirstSummaryNode → резюме 800-1200 слов
  │   └─ [Reflection Loop]
  │       ├─ ReflectionNode
  │       ├─ KeywordOptimizer + MediaCrawlerDB + SentimentAnalyzer
  │       └─ ReflectionSummaryNode → 1000-1500 слов
  └─ ReportFormattingNode → отчет 10000+ слов
  └─ Сохранить: insight_engine_streamlit_reports/report_xxx.md

T=1s-300s: (QueryEngine) - параллельно
  ├─ ReportStructureNode → 3-5 параграфов
  ├─ Для каждого параграфа:
  │   ├─ FirstSearchNode → выбор инструмента
  │   ├─ TavilyNewsAgency → поиск новостей
  │   └─ FirstSummaryNode → резюме 800-1200 слов
  └─ ReportFormattingNode → отчет 10000+ слов
  └─ Сохранить: query_engine_streamlit_reports/report_xxx.md

T=1s-300s: (MediaEngine) - параллельно
  ├─ ReportStructureNode → 3-5 параграфов
  ├─ Для каждого параграфа:
  │   ├─ FirstSearchNode → выбор инструмента
  │   ├─ BochaMultimodalSearch → мультимодальный поиск
  │   ├─ FirstSummaryNode → резюме 800-1200 слов
  │   └─ [Reflection Loop]
  │       ├─ ReflectionNode
  │       ├─ BochaMultimodalSearch
  │       └─ ReflectionSummaryNode → 1000-1500 слов
  └─ ReportFormattingNode → отчет 10000+ слов
  └─ Сохранить: media_engine_streamlit_reports/report_xxx.md

T=1s-300s: (ForumEngine) - параллельно фоново
  ├─ Детектирует FirstSummaryNode в логах
  ├─ Очищает forum.log
  ├─ Начинает извлечение выступлений агентов
  ├─ Каждые 5 выступлений → вызов HOST модератора
  └─ Сохраняет все в logs/forum.log

T=300s+: (ReportEngine)
  ├─ Проверяет наличие новых файлов от трех агентов
  ├─ Когда все готовы:
  │   ├─ TemplateSelectionNode → выбор шаблона
  │   ├─ HTMLGenerationNode → генерация HTML
  │   │   • Интеграция трех отчетов
  │   │   • Добавление forum.log данных
  │   │   • Создание Chart.js визуализаций
  │   │   • Стилизация CSS
  │   │   • JavaScript интерактивность
  │   └─ Сохранить: reports/final_report_xxx.html (30000+ слов)
  └─ ГОТОВО!

T=ежедневно 02:00 AM: (MindSpider) - автономный процесс
  ├─ BroadTopicExtraction
  │   ├─ Получить hot news с 5 источников
  │   ├─ TopicExtractor (LLM) → ключевые слова + резюме
  │   └─ Сохранить в БД: daily_topics, extracted_keywords
  └─ DeepSentimentCrawling
      ├─ Для каждого ключевого слова:
      │   ├─ Для каждой из 7 платформ:
      │   │   ├─ Краулить топ 50 постов
      │   │   ├─ Для каждого поста: краулить топ 50 комментариев
      │   │   └─ Сохранить в БД: {platform}_posts, {platform}_comments
      └─ ГОТОВО (БД пополнена для InsightEngine)
```

### 7.2 Ключевые метрики системы

```
┌─────────────────────────────────────────────────────────────────────┐
│                         МЕТРИКИ ПРОИЗВОДИТЕЛЬНОСТИ                   │
└─────────────────────────────────────────────────────────────────────┘

INSIGHTENGINE:
  • Время выполнения: 5-15 минут
  • Запросов к БД: 50-150 (в зависимости от ключевых слов)
  • LLM вызовов: 15-30 (structure + 5 paragraphs × 3-5 prompts)
  • Объем данных: 1000-5000 записей из БД
  • Итоговый отчет: 10 000+ слов

QUERYENGINE:
  • Время выполнения: 3-8 минут
  • API запросов: 20-50 (Tavily News)
  • LLM вызовов: 10-20 (structure + 3-5 paragraphs × 2 prompts)
  • Объем данных: 50-200 новостных статей
  • Итоговый отчет: 10 000+ слов

MEDIAENGINE:
  • Время выполнения: 4-10 минут
  • API запросов: 30-70 (Bocha Multimodal)
  • LLM вызовов: 15-25 (structure + 3-5 paragraphs × 3-4 prompts)
  • Объем данных: 100-500 веб-страниц + изображения
  • Итоговый отчет: 10 000+ слов

FORUMENGINE:
  • Работает: постоянно (фоновый поток)
  • Задержка: <1 секунда (обнаружение новых выступлений)
  • LLM вызовов: 5-15 (модерация каждые 5 выступлений)
  • Объем логов: 5-20 KB forum.log за сессию

REPORTENGINE:
  • Время выполнения: 2-5 минут
  • LLM вызовов: 2 (template selection + HTML generation)
  • Входные данные: 3 × 10 000 слов + forum.log
  • Итоговый отчет: 30 000+ слов (HTML)

MINDSPIDER:
  • Время выполнения: 2-6 часов (ежедневный сбор)
  • Краулинг запросов: 5000-20000 (в зависимости от ключевых слов)
  • LLM вызовов: 1 (TopicExtractor)
  • Собрано записей: 10 000-50 000 постов + комментариев в день
  • Рост БД: ~500 MB в день
```

---

**Дата создания:** 2025-11-11  
**Версия:** BettaFish Multi-Agent Sentiment Analysis System v1.0  
**Файл:** AGENT_PIPELINE_SCHEMAS.md

